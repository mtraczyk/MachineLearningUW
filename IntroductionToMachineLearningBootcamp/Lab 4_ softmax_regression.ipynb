{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vMn5wiRt7zn"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_xdi0AUN81t"
      },
      "source": [
        "# Softmax regression\n",
        "\n",
        "In this exercise you will train a softmax regression model to recognize handwritten digits.\n",
        "  \n",
        "The general setup is as follows:\n",
        "* we are given a set of pairs $(x, y)$, where $x \\in R^D$ is a vector of real numbers representing the features, and $y \\in \\{1,...,c\\}$ is the target (in our case we have ten classes, so $c=10$),\n",
        "* for a given $x$ we model the probability of $y=j$ by $$h(x)_j=p_j = \\frac{e^{w_j^Tx}}{\\sum_{i=1}^c e^{w_i^Tx}},$$\n",
        "* to find the right $w$ we will optimize the so called multiclass log loss:\n",
        "$$L(y,p) = \\log{p_y},$$\n",
        "$$J(w) = -\\frac{1}{n}\\sum_{i=1}^n L(y_i,h(x)),$$\n",
        "* with the loss function in hand we can improve our guesses iteratively:\n",
        "    * $w_{ij}^{t+1} = w_{ij}^t - \\text{step_size} \\cdot \\frac{\\partial J(w)}{\\partial w_{ij}}$,\n",
        "* we can end the process after some predefined number of epochs (or when the changes are no longer meaningful)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3yuYp69N810"
      },
      "source": [
        "Let's start with importing the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZOx1cckN814"
      },
      "source": [
        "!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz\n",
        "!pip install plotly==5.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfggfnt5N82K"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_mnist(path='mnist.npz'):\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjFxWDLnN82X"
      },
      "source": [
        "Let's take a look at the data. In the \"x\" arrays you'll find the images (encoded as pixel intensities) and in the \"y\" ones you'll find the labels (one-hot encoded)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxPEnhO_N82d"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_train[:10])\n",
        "print(y_train[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yATia4LaN82n"
      },
      "source": [
        "Now let us see the data in a more human way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXk-h0YuN82q"
      },
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "num_samples = 10\n",
        "plots = make_subplots(rows=1, cols=num_samples)\n",
        "\n",
        "for i in range(num_samples):\n",
        "  a = x_train[i, :].reshape(28,28)\n",
        "  img = go.Heatmap(z=a, colorscale='gray')\n",
        "  plots.add_trace(img, row=1, col=i+1)\n",
        "\n",
        "plots.update_yaxes(autorange='reversed', scaleanchor='x', constrain='domain')\n",
        "plots.update_xaxes(constrain='domain')\n",
        "plots.update_traces(showscale=False)\n",
        "plots.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgFd3QrRN82w"
      },
      "source": [
        "Next, we prepare $X$ and $y$ variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQKol9KmN82z"
      },
      "source": [
        "X = x_train[:4000]\n",
        "y = y_train[:4000]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpAv_SngN83A"
      },
      "source": [
        "To train the model we will (obviously) use gradient descent. Inside the loop we need a method to compute the gradients. Let's start with implementing it, together with some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUCxU7qEN83D"
      },
      "source": [
        "# We will store the weights in a D x c matrix, where D is the number of features, and c is the number of classes\n",
        "#weights = (...) # TODO: Fill in, be sure to have the right shape!\n",
        "weights = np.zeros([X.shape[1], 10])\n",
        "\n",
        "\n",
        "def softmax(z):\n",
        "    ########################################\n",
        "    # TODO: implement the softmax function #\n",
        "    ########################################\n",
        "\n",
        "\n",
        "def predict(weights, X):\n",
        "    ###################################\n",
        "    # TODO: compute the probabilities #\n",
        "    ###################################\n",
        "\n",
        "def compute_loss_and_gradients(weights, X, y, l2_reg):\n",
        "    #############################################################################\n",
        "    # TODO: compute loss and gradients, don't forget to include regularization! #\n",
        "    #############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E65eXzFVN83Q"
      },
      "source": [
        "We are now in position to complete the training pipeline.\n",
        "\n",
        "If you have problems with convergence, be sure to check the gradients numerically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SyqXq54QN83W"
      },
      "source": [
        "l2_reg = 0.5\n",
        "n_epochs = 250\n",
        "lr = 0.05\n",
        "t = 0.99\n",
        "\n",
        "losses = []\n",
        "for i in range(n_epochs):\n",
        "    loss, grad = compute_loss_and_gradients(weights, X, y, l2_reg)\n",
        "    losses.append(loss)\n",
        "\n",
        "    weights -= lr * grad\n",
        "    lr *= t\n",
        "\n",
        "fig = px.line(x=range(1,n_epochs+1), y=losses)\n",
        "layout = go.Layout(xaxis_title=\"Epoch\", yaxis_title='Loss')\n",
        "fig.update_layout(layout)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ5RVGfaN83j"
      },
      "source": [
        "Now compute your accuracy on the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkPf223hN83q"
      },
      "source": [
        "acc = np.mean(predict(weights, x_train).argmax(axis=1) == y_train.argmax(axis=1))\n",
        "print(\"Train accuracy: \", acc)\n",
        "acc = np.mean(predict(weights, x_test).argmax(axis=1) == y_test.argmax(axis=1))\n",
        "print(\"Test accuracy: \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcHaryrN83v"
      },
      "source": [
        "We can also visualize the weights learned by our algorithm. Try to anticipate the result before executing the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC30gyHHN83y"
      },
      "source": [
        "num_samples = 10\n",
        "plots = make_subplots(rows=1, cols=num_samples)\n",
        "\n",
        "for i in range(num_samples):\n",
        "  a = weights[:, i].reshape(28,28)\n",
        "  img = go.Heatmap(z=a, colorscale='gray')\n",
        "  plots.add_trace(img, row=1, col=i+1)\n",
        "\n",
        "plots.update_yaxes(autorange='reversed', scaleanchor='x', constrain='domain')\n",
        "plots.update_xaxes(constrain='domain')\n",
        "plots.update_traces(showscale=False)\n",
        "plots.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OsgljzA_N837"
      },
      "source": [
        "Note that we only used a small portion of the data to develop the model. Now, implement the training on full data. Also, validate your model properly and find a good value for `l2_reg` hyperparameter. Try to experiment with `batch_size`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgOzhRzhu8g5"
      },
      "source": [
        "################################################\n",
        "# TODO: implement the proper training pipeline #\n",
        "################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBbyHpTCt_na"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>"
      ]
    }
  ]
}