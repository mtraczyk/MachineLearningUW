{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a0c19f",
   "metadata": {
    "id": "91a0c19f"
   },
   "source": [
    "# Bigger, Better, Faster or Rainbow DQN v2\n",
    "### by: Mateusz Doliński, Mateusz Olko\n",
    "### special thanks for the inspiration: Michał Nauman\n",
    "\n",
    "In this homework we will expand upon on the Deep Q-Network (DQN) algorithm [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). DQN has been successfully applied to a wide range of environments and has demonstrated strong performance on many tasks. However, several challenges and limitations to the DQN that have been identified in the literature:\n",
    "\n",
    "1. Sample complexity - DQN can require a large number of samples to learn effectively, especially in environments with high-dimensional state spaces or a large number of possible actions\n",
    "2. Convergence - DQN is known to converge to the optimal solution under certain conditions, but the convergence properties of the algorithm are not well understood and it is not guaranteed to converge in all cases\n",
    "3. Overestimation - DQN is known to sometimes overestimate the Q-values of certain actions, which can lead to suboptimal behavior\n",
    "4. Sensitivity to hyperparameters - DQN can be sensitive to the choice of hyperparameters, such as the learning rate, the discount factor, and the exploration scheme.\n",
    "\n",
    "Last year the task was to implement the rainbow algorithm [(Hessel 2017)](https://arxiv.org/pdf/1710.02298.pdf). The algorithm is a combination of several techniques for improving the performance of the DQN algorithm, which was originally proposed by DeepMind. Rainbow algorithm is able to improve the sample efficiency, stability and  performance of the DQN algorithm. The improvements include u.a.:\n",
    "\n",
    "1. Double DQN\n",
    "2. N-step Q-value estimation\n",
    "3. Noisy Layer exploration\n",
    "4. Dueling DQN\n",
    "5. Prioritized experience replay\n",
    "\n",
    "Reinforcement Learning is still a new branch of research and paradigms tend to raise and fall quite frequently. In this case, the recent literature proved that the last 3 improvementx of rainbow DQN are not as good as advertised. In their place [(Schwarzer et al. 2023)](https://arxiv.org/pdf/2305.19452.pdf) introduced other improvements that add up to the new Bigger, Better, Faster (BBF) algorithm.\n",
    "\n",
    "In this homework, you will augment a baseline DQN implementation with components of BBF except for distributional Q-learning. To test our implementations, we will use the Lunar Lander environment with a budget of 40000 enironment steps and 30000 Q-network weight updates. You will also implement the evaluation as in [(Agarwal et al. 2022)](https://arxiv.org/pdf/2108.13264.pdf).\n",
    "\n",
    "## Homework scenario and grading\n",
    "\n",
    "You are provided with a baseline implementation of the DQN. Your job is to expand it with the following modules:\n",
    "\n",
    "1. N-step Q-value estimation with horizon annealing **2 points**\n",
    "2. Discount annealing **1 point**\n",
    "3. Q-network resets **2 points**\n",
    "4. BBF **2 points**\n",
    "5. IQM evaluation **3 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z_2HKKYIdygC",
   "metadata": {
    "id": "Z_2HKKYIdygC"
   },
   "outputs": [],
   "source": [
    "! pip install swig\n",
    "! pip install gymnasium[box2d]>=0.29.0\n",
    "! pip install typeguard==2.13.3\n",
    "! pip install torchtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0728d82",
   "metadata": {
    "id": "d0728d82"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass, field\n",
    "from functools import cached_property\n",
    "from typing import Any, Callable\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtyping import TensorType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e4c1f",
   "metadata": {
    "id": "094e4c1f"
   },
   "source": [
    "You are given a simple class for holding the hyperparameters (do not change those!) and a helper functions for setting seeds and orthogonal weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1883162",
   "metadata": {
    "id": "e1883162"
   },
   "outputs": [],
   "source": [
    "# do NOT change!\n",
    "@dataclass(frozen=True)\n",
    "class Hyperparameters:\n",
    "    capacity: int = 10000\n",
    "    init_steps: int = 10000\n",
    "    total_timesteps = 40000\n",
    "    batch_size: int = 128\n",
    "    hidden_dim: int = 128\n",
    "    optimizer_params: dict[str, Any] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"lr\": 7e-4,\n",
    "            \"eps\": 1e-5,\n",
    "            \"weight_decay\": 1e-3,\n",
    "        }\n",
    "    )\n",
    "    samples: int = 3\n",
    "    target_update_freq: int = 50\n",
    "    evaluate_freq: int = 1000\n",
    "    evaluate_samples: int = 5\n",
    "\n",
    "    anneal_steps: int = 30000\n",
    "\n",
    "    init_discount: float = 0.8\n",
    "    final_discount: float = 0.99\n",
    "\n",
    "    init_epsilon: float = 0.1\n",
    "    final_epsilon: float = 0.05\n",
    "\n",
    "    init_nstep: int = 10\n",
    "    final_nstep: int = 3\n",
    "    anneal_nstep_freq: int = 2000\n",
    "\n",
    "    reset_freq: int = 30100\n",
    "    replay_ratio: int = 2\n",
    "\n",
    "    gym_id: str = \"LunarLander-v2\"\n",
    "    cuda: bool = True\n",
    "\n",
    "    @cached_property\n",
    "    def state_dim(self) -> int:\n",
    "        env = gym.make(self.gym_id)\n",
    "        return env.observation_space.shape[0]\n",
    "\n",
    "    @cached_property\n",
    "    def action_dim(self) -> int:\n",
    "        env = gym.make(self.gym_id)\n",
    "        return env.action_space.n\n",
    "\n",
    "    @cached_property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() and self.cuda else \"cpu\")\n",
    "\n",
    "\n",
    "hyperparameters = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97603d29",
   "metadata": {
    "id": "97603d29"
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(\n",
    "    env: gym.wrappers.time_limit.TimeLimit,\n",
    "    seed: int,\n",
    ") -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "\n",
    "def weight_init(model: nn.Module) -> None:\n",
    "    if isinstance(model, nn.Linear):\n",
    "        nn.init.orthogonal_(model.weight.data)\n",
    "        model.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32176d86",
   "metadata": {
    "id": "32176d86"
   },
   "source": [
    "## 0. Baseline DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed14f18",
   "metadata": {
    "id": "8ed14f18"
   },
   "source": [
    "Deep Q-Network (DQN) [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a reinforcement learning algorithm that uses a deep neural network to learn a Q-function, which is a function that estimates the expected return for taking a given action in a given state. The goal of the DQN algorithm is to learn a policy that maximizes the expected return by learning the Q-function and selecting the action with the highest estimated return in each state.\n",
    "\n",
    "The DQN algorithm consists of two main components: a Q-network and an experience buffer. The Q-network is a deep neural network that takes in a state as input and outputs the estimated Q-values for each possible action. The experience buffer is a data structure that stores a set of experiences. The DQN algorithm works by interacting with the environment and storing the experiences in the experience buffer. The Q-network is then trained using a mini-batch of experiences uniformly sampled from the experience buffer. This process is known as experience replay and is used to decorrelate the experiences and to stabilize the learning process. The Q-network is updated using the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\theta} = \\frac{1}{B} \\sum_{i=1}^{B} \\bigl( \\mathrm{TD}~(s_i, a_i, s^{'}_{i}) \\bigr)^{2}\n",
    "$$\n",
    "\n",
    "With:\n",
    "\n",
    "$$\n",
    "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\underset{a^{'}_{i} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i}^{'},a_{i}^{'}) \\bigr)\n",
    "$$\n",
    "\n",
    "Where $Q_{\\theta}$ and $\\bar{Q}_{\\theta}$ denote learned and target Q-networks respectively. The target network is a copy of the Q-network that is updated less frequently, and using it to compute the target Q-values helps to stabilize the learning process and improve the performance of the DQN algorithm. Note that to increase stability of training we use Huber loss (smooth_l1_loss) instead of L2.\n",
    "\n",
    "There are several ways to incorporate exploration into the DQN algorithm. One common method is to use an $\\epsilon$-greedy exploration strategy, where the agent takes a random action with probability $\\epsilon$ and takes the action with the highest estimated Q-value with probability $1 - \\epsilon$. The value of $\\epsilon$ is typically decreased over time, so that the agent initially explores more and then gradually shifts towards exploitation as it learns more about the environment.\n",
    "\n",
    "Below, you will find the implementaiton of all the components of a basic DQN:\n",
    "\n",
    "**Experience buffer** - a data structure that stores a set of transitions, where a transition is typically represented as a tuple $(s, a, r, s', t)$, where $s$ is the state, $a$ is the action taken in state $s$, $r$ is the reward received by performing $a$ in $s$ and getting to $s'$, $s'$ is the new state observed after performing $a$ in $s$ and $t$ is the termination boolean (true if $s'$ is terminal). The **ExperienceBuffer** class below is using NumPy arrays has two methods:\n",
    "\n",
    "1. *add* - adds transition to the buffer\n",
    "2. *sample* - samples a batch of transitions from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e147c5",
   "metadata": {
    "id": "53e147c5"
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hyperparameters: Hyperparameters,\n",
    "    ) -> None:\n",
    "        self.states = np.zeros((hyperparameters.capacity, hyperparameters.state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((hyperparameters.capacity, 1), dtype=np.int64)\n",
    "        self.rewards = np.zeros((hyperparameters.capacity, 1), dtype=np.float32)\n",
    "        self.next_states = np.zeros((hyperparameters.capacity, hyperparameters.state_dim), dtype=np.float32)\n",
    "        self.terminals = np.zeros((hyperparameters.capacity, 1), dtype=np.int64)\n",
    "        self.full = False\n",
    "        self.idx = 0\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        terminal: bool,\n",
    "    ) -> None:\n",
    "        self.states[self.idx, :] = state\n",
    "        self.actions[self.idx, :] = action\n",
    "        self.rewards[self.idx, :] = reward\n",
    "        self.next_states[self.idx, :] = next_state\n",
    "        self.terminals[self.idx, :] = 1 if terminal else 0\n",
    "        self.idx += 1\n",
    "        if self.idx == self.hyperparameters.capacity:\n",
    "            self.full = True\n",
    "            self.idx = 0\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "    ) -> tuple[\n",
    "        TensorType[\"batch\", \"state_dim\"],\n",
    "        TensorType[\"batch\", 1],\n",
    "        TensorType[\"batch\", 1],\n",
    "        TensorType[\"batch\", \"state_dim\"],\n",
    "        TensorType[\"batch\", 1],\n",
    "    ]:\n",
    "        idx = (\n",
    "            np.random.permutation(self.hyperparameters.capacity)[: self.hyperparameters.batch_size]\n",
    "            if self.full\n",
    "            else np.random.permutation(self.idx - 1)[: self.hyperparameters.batch_size]\n",
    "        )\n",
    "        states = torch.from_numpy(self.states[idx]).to(self.hyperparameters.device)\n",
    "        actions = torch.from_numpy(self.actions[idx]).to(self.hyperparameters.device)\n",
    "        rewards = torch.from_numpy(self.rewards[idx]).to(self.hyperparameters.device)\n",
    "        next_states = torch.from_numpy(self.next_states[idx]).to(self.hyperparameters.device)\n",
    "        terminals = torch.from_numpy(self.terminals[idx]).long().to(self.hyperparameters.device)\n",
    "        return states, actions, rewards, next_states, terminals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdcd965",
   "metadata": {
    "id": "9fdcd965"
   },
   "source": [
    "**QNetwork** - a simple dense MLP. Note the output size being equal to the amount of actions in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb7c14",
   "metadata": {
    "id": "e3fb7c14"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hyperparameters: Hyperparameters,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hyperparameters.state_dim, hyperparameters.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyperparameters.hidden_dim, hyperparameters.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyperparameters.hidden_dim, hyperparameters.action_dim),\n",
    "        )\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: TensorType[\"batch\", \"state_dim\"],\n",
    "        ) -> TensorType[\"batch\", \"actions_dim\"]:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed7a71",
   "metadata": {
    "id": "03ed7a71"
   },
   "source": [
    "**DQN agent** - implementation of the callbacks required to learn the DQN algorithm. The class has following methods:\n",
    "\n",
    "1. *get_action* - returns action in given state using $\\epsilon$-greedy\n",
    "2. *anneal* - reduces the value of $\\epsilon$ dependent on the training step\n",
    "3. *update* - samples a batch of transitions from the experience buffer and performs a DQN update\n",
    "4. *update_target* - performs a hard update on the target Q network $\\bar{Q}_{\\theta}$\n",
    "5. *evaluate* - performs evaluation of the agent with a greedy policy\n",
    "6. *reset* - resets the agent (used between seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b71037",
   "metadata": {
    "id": "28b71037"
   },
   "outputs": [],
   "source": [
    "class DQNBaseline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hyperparameters: Hyperparameters,\n",
    "    ) -> None:\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.buffer = ExperienceBuffer(self.hyperparameters)\n",
    "        self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "        self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
    "\n",
    "        self.epsilon = self.hyperparameters.init_epsilon\n",
    "        self.discount = self.hyperparameters.final_discount\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.buffer = ExperienceBuffer(self.hyperparameters)\n",
    "        self.epsilon = self.hyperparameters.init_epsilon\n",
    "        self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "        self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
    "\n",
    "    def get_action(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        exploration: bool = True,\n",
    "    ) -> int:\n",
    "        with torch.no_grad():\n",
    "            return (\n",
    "                np.random.randint(self.hyperparameters.action_dim)\n",
    "                if np.random.sample() < self.epsilon and exploration\n",
    "                else torch.argmax(self.q_net(state)).item()\n",
    "            )\n",
    "\n",
    "    def anneal(\n",
    "        self,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        self.epsilon = (\n",
    "            ((self.hyperparameters.final_epsilon - self.hyperparameters.init_epsilon) / self.hyperparameters.anneal_steps) * step\n",
    "            + self.hyperparameters.init_epsilon\n",
    "            if step < self.hyperparameters.anneal_steps\n",
    "            else self.epsilon\n",
    "        )\n",
    "\n",
    "    def update(self) -> None:\n",
    "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
    "        with torch.no_grad():\n",
    "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (1 - terminals) * self.discount * q_ns\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target(self) -> None:\n",
    "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        samples: int,\n",
    "    ) -> float:\n",
    "        with torch.no_grad():\n",
    "            env_test = gym.make(self.hyperparameters.gym_id, max_episode_steps=1000)\n",
    "            eval_rewards = np.zeros((samples,))\n",
    "            for i in range(samples):\n",
    "                state, _ = env_test.reset()\n",
    "                episode_reward = 0\n",
    "                while True:\n",
    "                    action = self.get_action(torch.tensor(state).unsqueeze(0).to(self.hyperparameters.device), False)\n",
    "                    next_state, reward, terminal, truncated, _ = env_test.step(action)\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    if terminal or truncated:\n",
    "                        # eval_reward += episode_reward / samples\n",
    "                        eval_rewards[i] = episode_reward\n",
    "                        break\n",
    "        return eval_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539a644",
   "metadata": {
    "id": "f539a644"
   },
   "source": [
    "Finally, you are presented with the training loop for the DQN agents (do NOT change this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a9330",
   "metadata": {
    "id": "385a9330"
   },
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    hyperparameters: Hyperparameters,\n",
    "    agent: DQNBaseline,\n",
    ") -> np.ndarray:\n",
    "    results = np.zeros(\n",
    "        (\n",
    "            hyperparameters.total_timesteps // hyperparameters.evaluate_freq,\n",
    "            hyperparameters.samples,\n",
    "            hyperparameters.evaluate_samples\n",
    "        )\n",
    "    )\n",
    "    for seed_idx, seed in enumerate(range(hyperparameters.samples)):\n",
    "        env = gym.make(hyperparameters.gym_id, max_episode_steps=1000)\n",
    "        agent.reset()\n",
    "        set_seed_everywhere(env, seed)\n",
    "        state, _ = env.reset()\n",
    "        for step in range(hyperparameters.total_timesteps):\n",
    "            if step == hyperparameters.init_steps:\n",
    "                start_time = time.time()\n",
    "            action = agent.get_action(\n",
    "                torch.tensor(state).unsqueeze(0).to(hyperparameters.device)\n",
    "            )\n",
    "            next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "            agent.buffer.add(state, action, reward, next_state, terminal or truncated)\n",
    "            agent.anneal(step)\n",
    "            state = next_state\n",
    "            if step >= hyperparameters.init_steps:\n",
    "                for update_num in range(hyperparameters.replay_ratio):\n",
    "                    agent.update()\n",
    "                    if (\n",
    "                        step * hyperparameters.replay_ratio + update_num + 1\n",
    "                    ) % hyperparameters.target_update_freq == 0:\n",
    "                        agent.update_target()\n",
    "                if (step + 1) % hyperparameters.evaluate_freq == 0:\n",
    "                    eval_rewards = agent.evaluate(hyperparameters.evaluate_samples)\n",
    "                    results[step // hyperparameters.evaluate_freq, seed] = eval_rewards\n",
    "                    print(\n",
    "                        \"\\rSample number: {} Step: {} Evaluation reward: {:.2f} Samples per second: {:}\".format(\n",
    "                            seed_idx + 1,\n",
    "                            step,\n",
    "                            eval_rewards.mean(),\n",
    "                            int(\n",
    "                                (step - hyperparameters.init_steps)\n",
    "                                / (time.time() - start_time)\n",
    "                            ),\n",
    "                        ),\n",
    "                        end=\"\",\n",
    "                    )\n",
    "            if terminal:\n",
    "                state, _ = env.reset()\n",
    "                episode_reward = 0\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff186311",
   "metadata": {
    "id": "ff186311"
   },
   "source": [
    "The training of the baseline DQN agent is implemented in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d29b1",
   "metadata": {
    "id": "938d29b1"
   },
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "agent = DQNBaseline(hyperparameters)\n",
    "results_dqn = train_agent(hyperparameters, agent)\n",
    "results_dict[\"DQNBaseline\"] = results_dqn\n",
    "results_dqn.mean(1)[10:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376e3e2",
   "metadata": {
    "id": "3376e3e2"
   },
   "source": [
    "Below the proper task begins:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004bf3a3",
   "metadata": {
    "id": "004bf3a3"
   },
   "source": [
    "## 1. N-step Q-value estimation with horizon annealing\n",
    "\n",
    "$N$-step TD ($\\mathrm{TD}_{n}$) was introduced long before neural network based RL. In regular TD, we supervise the Q-network with single-step reward summed with highest Q-value of the next state. In contrast to that, $\\mathrm{TD}_{n}$ accumulated rewards over $n$ steps and sums it with the highest Q-value of the state that occured after $n$ steps [(Sutton 1988)](http://incompleteideas.net/papers/sutton-88-with-erratum.pdf). Double DQN $\\mathrm{TD}_{n}$ loss is defined by:\n",
    "\n",
    "$$\n",
    "\\mathrm{TD}_{n}(s_i, a_i, s^{'}_{i+n}) = Q_{\\theta}~(s_i,a_i) - \\biggl(\\sum_{k=0}^{n-1} \\gamma^{k} ~ r_{(s_{i+k},a_{i+k},s_{i+k}^{'})} + \\gamma^{n} \\underset{a^{'}_{i+n}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i+n}^{'},a_{i+n}^{'}) \\biggr)\n",
    "$$\n",
    "\n",
    "The horizon hyperparameter (n) is going to be annealed from the `hyperparameters.init_nstep` to `hyperparameters.final_nstep`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448764d",
   "metadata": {
    "id": "f448764d"
   },
   "source": [
    "### 1.1 Implement NStepExperienceBuffer\n",
    "\n",
    "Implementing $\\mathrm{TD}_{n}$ requires changes to the ExperienceBuffer class. We will implement those changes using the **deque** module. This module will store $n$ of the most recent transitions, and will act as a middleware between agent and buffers main storage. As compared to single step reward and $s_{i}^{'}$ stored by the simple ExperienceBuffer, the main storage of this upgraded buffer should store $n$ step rewards and $s_{i+n}^{'}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12560df7",
   "metadata": {
    "id": "12560df7"
   },
   "outputs": [],
   "source": [
    "class NStepExperienceBuffer(ExperienceBuffer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hyperparameters: Hyperparameters,\n",
    "        ) -> None:\n",
    "        super().__init__(hyperparameters)\n",
    "        self.memories = deque(maxlen=self.hyperparameters.init_nstep)\n",
    "        self.nstep = self.hyperparameters.init_nstep\n",
    "        self.discount = self.hyperparameters.final_discount\n",
    "\n",
    "    def set_nstep(\n",
    "            self,\n",
    "            value: int\n",
    "        ) -> None:\n",
    "        ############### TODO ###############\n",
    "        self.nstep = value\n",
    "        ####################################\n",
    "\n",
    "    def get_nstep(self) -> tuple[np.ndarray, int, float, np.ndarray, bool]:\n",
    "        ############### TODO ###############\n",
    "        pth_len = 0\n",
    "        while pth_len != self.nstep and len(self.memories) > 0:\n",
    "            a_state, a_action, a_reward, a_next_state, a_terminal = self.memories.popleft()\n",
    "            action, state = a_action, a_state\n",
    "            total_reward, cur_discount, pth_len = a_reward, 1., 1\n",
    "            terminal_ = 1 if a_terminal else 0\n",
    "            while pth_len != self.nstep and terminal_ != 1 and len(self.memories) > 0:\n",
    "                pth_len = pth_len + 1\n",
    "                a_state, a_action, a_reward, a_next_state, a_terminal = self.memories.popleft()\n",
    "                terminal_ = 1 if a_terminal else 0\n",
    "                cur_discount *= self.discount\n",
    "                total_reward += cur_discount * a_reward\n",
    "\n",
    "        reward, next_state, terminal = total_reward, a_next_state, a_terminal\n",
    "        if pth_len != self.nstep:\n",
    "            return None\n",
    "        ####################################\n",
    "        return state, action, reward, next_state, terminal\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        terminal: bool,\n",
    "    ) -> None:\n",
    "        terminal_ = 1 if terminal else 0\n",
    "        memory = (state, action, reward, next_state, terminal_)\n",
    "        self.memories.append(memory)\n",
    "        if len(self.memories) >= self.nstep:\n",
    "            ############### TODO ###############\n",
    "            transition = self.get_nstep()\n",
    "            if transition is not None:\n",
    "                state, action, reward, next_state, terminal = transition\n",
    "                self.states[self.idx, :] = state\n",
    "                self.actions[self.idx, :] = action\n",
    "                self.rewards[self.idx, :] = reward\n",
    "                self.next_states[self.idx, :] = next_state\n",
    "                self.terminals[self.idx, :] = 1 if terminal else 0\n",
    "            ####################################\n",
    "                self.idx += 1\n",
    "                if self.idx == self.hyperparameters.capacity:\n",
    "                    self.full = True\n",
    "                    self.idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d45601",
   "metadata": {
    "id": "34d45601"
   },
   "source": [
    "### 1.2: Implement and train N-step annealing DQN\n",
    "Implement **NStepAnnealing** by including the annealing step. It should decrease from the **hyperparameters.init_nstep** value to the **hyperparameters.final_nstep** value by 1 every **hyperparameters.anneal_nstep_freq** steps.\n",
    "\n",
    "Remember to properly set `nstep` parameter in all relevant attributes of the **NStepAnnealing** class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1cb4c6",
   "metadata": {
    "id": "0d1cb4c6"
   },
   "outputs": [],
   "source": [
    "class NStepAnnealing(DQNBaseline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hyperparameters: Hyperparameters,\n",
    "    ) -> None:\n",
    "        super().__init__(hyperparameters)\n",
    "        self.nstep = self.hyperparameters.init_nstep\n",
    "        self.buffer = NStepExperienceBuffer(hyperparameters)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        ############### TODO ###############\n",
    "        self.nstep = self.hyperparameters.init_nstep\n",
    "        self.buffer = NStepExperienceBuffer(self.hyperparameters)\n",
    "        ####################################\n",
    "\n",
    "    def set_nstep(\n",
    "            self,\n",
    "            value: int,\n",
    "        ) -> None:\n",
    "        ############### TODO ###############\n",
    "        self.nstep = value\n",
    "        self.buffer = NStepExperienceBuffer(self.hyperparameters)\n",
    "        self.buffer.set_nstep(value)\n",
    "        ####################################\n",
    "\n",
    "    def anneal(\n",
    "        self,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        super().anneal(step=step)\n",
    "        ############### TODO ###############\n",
    "        if (step + 1) % self.hyperparameters.anneal_nstep_freq == 0 and self.nstep > self.hyperparameters.final_nstep:\n",
    "            self.set_nstep(self.nstep - 1)\n",
    "        ####################################\n",
    "\n",
    "    def update(self) -> None:\n",
    "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
    "        ############### TODO ###############\n",
    "        with torch.no_grad():\n",
    "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (1 - terminals) * (self.discount ** self.nstep) * q_ns\n",
    "        ####################################\n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49e8b4",
   "metadata": {
    "id": "1a49e8b4"
   },
   "source": [
    "Launch the training of the NStepAnnealing DQN agent and observe difference in results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7900",
   "metadata": {
    "id": "e27e7900"
   },
   "outputs": [],
   "source": [
    "agent = NStepAnnealing(hyperparameters)\n",
    "results_dqn2 = train_agent(hyperparameters, agent)\n",
    "results_dict[\"NStepAnnealing\"] = results_dqn2\n",
    "results_dqn2.mean(1)[-10:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YtM0SoH2c2hb",
   "metadata": {
    "id": "YtM0SoH2c2hb"
   },
   "source": [
    "## 2. Discount annealing\n",
    "\n",
    "Remember that the loss in the baseline DQN is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{TD}~(s_i, a_i, s^{'}_{i}) = Q_{\\theta}~(s_i,a_i) - \\bigl(r_{(s_i,a_i,s_{i}^{'})} + \\gamma ~ \\underset{a^{'}_{i} \\sim \\bar{Q}_{\\theta}}{\\mathrm{max}} ~ \\bar{Q}_{\\theta}~(s_{i}^{'},a_{i}^{'}) \\bigr)\n",
    "$$\n",
    "\n",
    "This section includes implmenetation of annealing of the discount $\\gamma$ hyperperameter. It should lineary decrease from the **hyperparameters.init_discount** value to the **hyperparameters.final_discount** value over **anneal_steps** steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yb19-g1DaCl_",
   "metadata": {
    "id": "Yb19-g1DaCl_"
   },
   "outputs": [],
   "source": [
    "class DiscountAnnealing(DQNBaseline):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hyperparameters: Hyperparameters,\n",
    "        ) -> None:\n",
    "        super().__init__(hyperparameters)\n",
    "        self.discount = self.hyperparameters.init_discount\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        ############### TODO ###############\n",
    "        self.discount = self.hyperparameters.init_discount\n",
    "        ####################################\n",
    "\n",
    "    def set_discount(\n",
    "            self,\n",
    "            value: int,\n",
    "        ) -> None:\n",
    "        ############### TODO ###############\n",
    "        self.discount = value\n",
    "        ####################################\n",
    "\n",
    "    def anneal(\n",
    "        self,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        super().anneal(step=step)\n",
    "        ############### TODO ###############\n",
    "        if step <= self.hyperparameters.anneal_steps:\n",
    "            updated_discount = self.hyperparameters.init_discount - (self.hyperparameters.init_discount - self.hyperparameters.final_discount) * (step / self.hyperparameters.anneal_steps)\n",
    "            self.set_discount(updated_discount)\n",
    "        ####################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b0699",
   "metadata": {
    "id": "385b0699"
   },
   "source": [
    "Launch the training of the DiscountAnnealing DQN agent and observe difference in results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cVZGXODmhurZ",
   "metadata": {
    "id": "cVZGXODmhurZ"
   },
   "outputs": [],
   "source": [
    "agent = DiscountAnnealing(hyperparameters)\n",
    "results_dqn3 = train_agent(hyperparameters, agent)\n",
    "results_dict[\"DiscountAnnealing\"] = results_dqn3\n",
    "results_dqn3.mean(1)[-10:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66747342",
   "metadata": {
    "id": "66747342"
   },
   "source": [
    "## 3. Q-network resets\n",
    "The q-networks tend to overfit to initial, low quality data and loose plasticity over time. To overcome this problem reinitialize q-networks every `self.reset_freq` updates. Remeber to reset the optimizer parameters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35f21d",
   "metadata": {
    "id": "bd35f21d"
   },
   "outputs": [],
   "source": [
    "class Resets(DQNBaseline):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hyperparameters: Hyperparameters,\n",
    "        ) -> None:\n",
    "        super().__init__(hyperparameters)\n",
    "        self.reset_freq = hyperparameters.reset_freq\n",
    "        ############### TODO ###############\n",
    "        self.num_updates = 0\n",
    "        ####################################\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        ############### TODO ###############\n",
    "        self.num_updates = 0\n",
    "        ####################################\n",
    "\n",
    "    def update(self) -> None:\n",
    "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
    "        with torch.no_grad():\n",
    "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (1 - terminals) * self.discount * q_ns\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.num_updates += 1\n",
    "\n",
    "        ############### TODO ###############\n",
    "        if self.num_updates % self.reset_freq == 0:\n",
    "            self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "            self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "            self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "            self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
    "        ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea3aa2",
   "metadata": {
    "id": "7eea3aa2"
   },
   "source": [
    "Launch the training of the Resets DQN agent and observe difference in results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe46c0",
   "metadata": {
    "id": "66fe46c0"
   },
   "outputs": [],
   "source": [
    "agent = Resets(hyperparameters)\n",
    "results_dqn4 = train_agent(hyperparameters, agent)\n",
    "results_dict[\"Resets\"] = results_dqn4\n",
    "results_dqn4.mean(1)[-10:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7a09d",
   "metadata": {
    "id": "3bd7a09d"
   },
   "source": [
    "## 4. BBF\n",
    "\n",
    "In this section your task is to combine all the above ideas into a single DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b1a1d",
   "metadata": {
    "id": "ff5b1a1d"
   },
   "outputs": [],
   "source": [
    "class BBF(DQNBaseline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hyperparameters: Hyperparameters,\n",
    "    ) -> None:\n",
    "        super().__init__(hyperparameters)\n",
    "        self.nstep = self.hyperparameters.init_nstep\n",
    "        self.buffer = NStepExperienceBuffer(hyperparameters)\n",
    "        self.discount = self.hyperparameters.init_discount\n",
    "        self.reset_freq = hyperparameters.reset_freq\n",
    "        self.replay_ratio = hyperparameters.replay_ratio\n",
    "        self.num_updates = 0\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        super().reset()\n",
    "        self.buffer = NStepExperienceBuffer(self.hyperparameters)\n",
    "        self.nstep = self.hyperparameters.init_nstep\n",
    "        self.discount = self.hyperparameters.init_discount\n",
    "        self.num_updates = 0\n",
    "\n",
    "    def set_nstep(self, value: int) -> None:\n",
    "        ############### TODO ###############\n",
    "        self.nstep = value\n",
    "        ####################################\n",
    "\n",
    "    def set_discount(self, value: int) -> None:\n",
    "        ############### TODO ###############\n",
    "        self.discount = value\n",
    "        ####################################\n",
    "\n",
    "    def anneal(\n",
    "        self,\n",
    "        step: int,\n",
    "    ) -> None:\n",
    "        super().anneal(step=step)\n",
    "        # N-step\n",
    "        value = max(\n",
    "            self.hyperparameters.final_nstep,\n",
    "            min(\n",
    "                self.hyperparameters.init_nstep\n",
    "                - (step - self.hyperparameters.init_steps)\n",
    "                // self.hyperparameters.anneal_nstep_freq,\n",
    "                self.hyperparameters.init_nstep,\n",
    "            ),\n",
    "        )\n",
    "        self.set_nstep(value=value)\n",
    "        # Discount\n",
    "        value = (\n",
    "            (\n",
    "                (\n",
    "                    self.hyperparameters.final_discount\n",
    "                    - self.hyperparameters.init_discount\n",
    "                )\n",
    "                / self.hyperparameters.anneal_steps\n",
    "            )\n",
    "            * step\n",
    "            + self.hyperparameters.init_discount\n",
    "            if step < self.hyperparameters.anneal_steps\n",
    "            else self.discount\n",
    "        )\n",
    "        self.set_discount(value=value)\n",
    "\n",
    "    def reset_params(self) -> None:\n",
    "        ############### TODO ###############\n",
    "        self.q_net = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "        self.q_target = QNetwork(self.hyperparameters).to(self.hyperparameters.device)\n",
    "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), **self.hyperparameters.optimizer_params)\n",
    "        ####################################\n",
    "\n",
    "    def update(self) -> None:\n",
    "        states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
    "        with torch.no_grad():\n",
    "            q_ns = torch.max(self.q_target(next_states), dim=1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (1 - terminals) * self.discount**self.nstep * q_ns\n",
    "        self.optimizer.zero_grad()\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        loss = nn.functional.smooth_l1_loss(q_values, q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.num_updates += 1\n",
    "\n",
    "        ############### TODO ###############\n",
    "        if self.num_updates % self.reset_freq == 0:\n",
    "            self.reset_params()\n",
    "        ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6cc41",
   "metadata": {
    "id": "46b6cc41"
   },
   "source": [
    "Launch the training of the BBF DQN agent and observe difference in results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d0955",
   "metadata": {
    "id": "3d3d0955"
   },
   "outputs": [],
   "source": [
    "agent = BBF(hyperparameters)\n",
    "results_dqn5 = train_agent(hyperparameters, agent)\n",
    "results_dict[\"BBF\"] = results_dqn5\n",
    "results_dqn5.mean(1)[-10:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a78ec8",
   "metadata": {
    "id": "42a78ec8"
   },
   "source": [
    "## 5. Aggregate evaluation data and compute IQM metric\n",
    "At the end we ask you to present collected data according to highest standards in the area. Presented solution were suggested in the paper [Deep Reinforcement Learning at the Edge of the\n",
    "Statistical Precipice]().\n",
    "\n",
    "To aggregate performance we will use interquartile mean (IQM) instead of average.\n",
    "\n",
    "First implement IQM as an average of middle 50% of combined runs results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d1364",
   "metadata": {
    "id": "b88d1364"
   },
   "outputs": [],
   "source": [
    "def IQM(combined_runs: np.ndarray) -> float:\n",
    "    ############### TODO ###############\n",
    "    return np.mean(np.sort(combined_runs)[int(combined_runs.shape[-1] * 0.25):int(combined_runs.shape[-1] * 0.75)])\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7e136",
   "metadata": {
    "id": "17c7e136"
   },
   "source": [
    "Second we ask you to compute boostrap confidence interval to express uncertainty in the average performance. Follow these steps for implementation:\n",
    "1. Repeat the following process `n_samples` times: generate a sample of the same size as your original data by randomly sampling from it with replacement.\n",
    "2. On each iteration, calculate specified statistic (in this case, IQM) based on the generated sample.\n",
    "3. After completing all iterations, you will have a collection of `n_samples` IQM values. To construct a confidence interval, identify two quantiles, denoted as p1 and p2. These quantiles should be equidistant from the median (50%) and the distance between them should correspond to the desired confidence level. For instance, if the confidence level is 90%, set p1=0.05 and p2=0.95.\n",
    "4. Finally, return the p1-quantile and p2-quantile of your IQM values as the lower and upper bounds of your bootstrap confidence interval.\n",
    "\n",
    "To obtain maximum points for this task you must not use python \"for\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f352d",
   "metadata": {
    "id": "cd7f352d"
   },
   "outputs": [],
   "source": [
    "def bootstrap(\n",
    "    data: np.ndarray,\n",
    "    statistic: Callable,\n",
    "    n_resamples: int = 9999,\n",
    "    confidence_level: float = 0.95,\n",
    ") -> tuple[float, float]:\n",
    "    ############### TODO ###############\n",
    "    lower_quantile = (1 - confidence_level) / 2\n",
    "    upper_quantile = 1 - lower_quantile\n",
    "    samples = np.random.choice(data, size=(n_resamples, data.shape[0]), replace=True)\n",
    "    statistics = np.apply_along_axis(statistic, -1, samples)\n",
    "    return np.quantile(statistics, lower_quantile), np.quantile(statistics, upper_quantile)\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13829bf",
   "metadata": {
    "id": "e13829bf"
   },
   "source": [
    "Third implement aggregating function. For each method compute IQM nad confidence intervals using data from the last 10 evaluations, all eval runs and all seeds.\n",
    "Return DataFrame with the following columns: \"method_name\", \"IQM\", \"confidence_lower_bound\", \"confidence_upper_bound\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d691049",
   "metadata": {
    "id": "3d691049"
   },
   "outputs": [],
   "source": [
    "def aggregate_data(results: dict[str, np.ndarray]) -> pd.DataFrame:\n",
    "    ############### TODO ###############\n",
    "    data = []\n",
    "    for method_name, method_results in results.items():\n",
    "        lower_bound, upper_bound = bootstrap(method_results.flatten(), IQM)\n",
    "        data.append([method_name, IQM(method_results.flatten()), lower_bound, upper_bound])\n",
    "    return pd.DataFrame(data, columns=[\"method_name\", \"IQM\", \"confidence_lower_bound\", \"confidence_upper_bound\"])\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39b8c3",
   "metadata": {
    "id": "5a39b8c3"
   },
   "source": [
    "## 6. Plot collected results\n",
    "\n",
    "Use the provided function and replace the example data with your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27373aca",
   "metadata": {
    "id": "27373aca"
   },
   "outputs": [],
   "source": [
    "example_data = pd.DataFrame(\n",
    "    {\n",
    "        \"method_name\": [\"baseline\", \"discount_annealing\", \"n_step_annealing\", \"resets\", \"combined\"],\n",
    "        \"IQM\": [-24, 20, 23, 30, 40],\n",
    "        \"confidence_lower_bound\": [-30, 14, 20, 25, 33],\n",
    "        \"confidence_upper_bound\": [-20, 22, 27, 38, 44],\n",
    "    }\n",
    ")\n",
    "\n",
    "results_dict = {\"baseline\": results_dqn[-10:],\n",
    "                \"n_step_annealing\": results_dqn2[-10:],\n",
    "                \"discount_annealing\": results_dqn3[-10:],\n",
    "                \"resets\": results_dqn4[-10:],\n",
    "                \"combined\": results_dqn5[-10:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cf465",
   "metadata": {
    "id": "d22cf465"
   },
   "outputs": [],
   "source": [
    "def plot_results(data: pd.DataFrame) -> tuple[matplotlib.figure.Figure, matplotlib.axes._axes.Axes]:\n",
    "    assert data.shape == (5, 4)\n",
    "    assert set(data.columns) == set(\n",
    "        [\n",
    "            \"method_name\",\n",
    "            \"IQM\",\n",
    "            \"confidence_lower_bound\",\n",
    "            \"confidence_upper_bound\",\n",
    "        ]\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    colors = [\"skyblue\", \"lightgreen\", \"lightcoral\", \"orange\", \"lightpink\"]\n",
    "\n",
    "    for i, method in enumerate(data[\"method_name\"]):\n",
    "        mean = data.at[i, \"IQM\"]\n",
    "        lower_bound = data.at[i, \"confidence_lower_bound\"]\n",
    "        upper_bound = data.at[i, \"confidence_upper_bound\"]\n",
    "\n",
    "        rect_width = upper_bound - lower_bound\n",
    "\n",
    "        ax.plot(\n",
    "            [mean, mean],\n",
    "            [i - 0.4, i + 0.4],\n",
    "            color=\"black\",\n",
    "            linewidth=2,\n",
    "            label=\"Mean\" if i == 0 else \"\",\n",
    "        )\n",
    "\n",
    "        rect = plt.Rectangle(\n",
    "            (lower_bound, i - 0.4),\n",
    "            rect_width,\n",
    "            0.8,\n",
    "            color=colors[i],\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.set_yticks(\n",
    "        range(len(data)),\n",
    "        data[\"method_name\"],\n",
    "    )\n",
    "    ax.set_title(\"Results of each method with empirical confidence intervals\")\n",
    "\n",
    "    ax.grid(\n",
    "        axis=\"y\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9hu7jlrnw5f",
   "metadata": {
    "id": "S9hu7jlrnw5f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_results(aggregate_data(results_dict))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "6809863f01cf54cb8cf26991fcf8425a337722d7d35212492765d6bf47d2da35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
