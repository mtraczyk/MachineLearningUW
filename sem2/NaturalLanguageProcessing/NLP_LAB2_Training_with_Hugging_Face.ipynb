{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORxAF60ollb2"
      },
      "source": [
        "# NLP LAB2 - Training with Hugging Face\n",
        "\n",
        "This lab is focused on training models from scratch using the Hugging Face environment, touching a few topics important within NLP, like Tokenizers, in-context learning, and Encoder-Decoder models.\n",
        "\n",
        "We will try to implement a simple modulo calculator using a Transformer.\n",
        "\n",
        "To accelerate your training with a GPU, click on \"Change runtime type\" on the \"Runtime\" tab and select T4 GPU. Note, however, that Colab will allow you to use GPU only for a limited time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaL50VdJLp5f"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9js5fFz_moW"
      },
      "outputs": [],
      "source": [
        "%pip install gradio langchain openai datasets tokenizers datasets transformers numpy accelerate transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA6Oog8XLvrc"
      },
      "source": [
        "### Generate Data\n",
        "\n",
        "The code given below in the\n",
        " `generate_data_demo` function returns two strings, one representing a calculation using `length=3` digits with random operators from `['+', \"-\"]` and the other representing the results of this calculation modulo `modulo=100`.\n",
        "\n",
        " Example `generate_data_demo()`:\n",
        "\n",
        "```X: \"2 + 2 + 2\" Y: \"6\"```\n",
        "\n",
        "\n",
        "Your task is to write a `generate_data()` function that will expand this calculation step-by-step, starting from the left side:\n",
        "\n",
        "Example `generate_data()`:\n",
        "\n",
        "```X: \"2 + 2 + 2\" Y: \"4 + 2 = 6\"```\n",
        "\n",
        "\n",
        "But first, finish the notebook using demo implementation, by assigning `generate_data = generate_data_demo`.\n",
        "\n",
        "The goal of the notebook is to observe how differently these implementations behave."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANPn-UyEAlQN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "legal_ops = ['+', \"-\"]\n",
        "size = 10000\n",
        "length = 3\n",
        "max_num = 10\n",
        "modulo = 100\n",
        "test_size = 0.3\n",
        "\n",
        "def format_operation(numbers, ops):\n",
        "  return \" \".join(f\"{x} {op}\" for x, op in zip(numbers, list(ops) + [\"\"]))\n",
        "\n",
        "def eval_code(code):\n",
        "  return str(eval(f\"({code})%{modulo}\"))\n",
        "\n",
        "# DEMO generate_data\n",
        "def generate_data_demo():\n",
        "  numbers = np.random.randint(0, max_num, length)\n",
        "  ops = np.random.choice(legal_ops, length - 1)\n",
        "  formatted = format_operation(numbers, ops)\n",
        "  out = eval_code(formatted)\n",
        "  return formatted, out\n",
        "\n",
        "# Implement a generate_data function that has the same interface as generate_data_demo function\n",
        "## INSERT YOUR CODE HERE  ---------------------\n",
        "\n",
        "## END OF CODE ---------------------\n",
        "\n",
        "#generate_data = generate_data_demo\n",
        "# Uncomment the line above for a demo calculator\n",
        "\n",
        "generate_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dezi2k7DAnsR"
      },
      "source": [
        "#### Create a Hugging Face Dataset\n",
        "\n",
        "Sample data using the generator and create a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYN7EJ73cYHw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDQDLhIAcYF0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV-fAbp7AoF4"
      },
      "outputs": [],
      "source": [
        "# this ensures data does not repeat and therefore does not leak from train to test\n",
        "raw_dataset = list({generate_data() for i in range(size)})\n",
        "\n",
        "x_data, y_data = zip(*raw_dataset)\n",
        "dataset = Dataset.from_dict({\"text\":x_data, \"out\":y_data})\n",
        "dataset = dataset.train_test_split(test_size=test_size)\n",
        "print(f\"Train size = {len(dataset['train'])}, test size = {len(dataset['test'])}\")\n",
        "dataset[\"train\"][10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3V2-WE0Lotl"
      },
      "source": [
        "### Create the Tokenizer\n",
        "\n",
        "\n",
        "As a first step, we need to tokenize our input string into a series of discrete tokens. We're going to use a Byte Pair Encoding Tokenizer, explained in the lecture, that is trained on a given dataset. Moreover, to train our model to work with sequences of different lengths (there is no guarantee how BPE will tokenize our dataset), we need to add a few special tokens:\n",
        "- `[EOS]` a special token inserted at the end of the desired output, that is used to end the variable sequence length generation process.\n",
        "- `[UNK]` or tokens unseen during the training.\n",
        "- `[CLS]` which is a token separating the encoder and the decoder part.\n",
        "- `[PAD]`  a padding token, inserted after `[EOS]` to match the desired sequence length in a batch, actually masked during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skndSjsY_o49"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[EOS]\", \"[UNK]\", \"[CLS]\", \"[PAD]\"])\n",
        "\n",
        "tokenizer.train_from_iterator(raw_dataset, trainer=trainer)\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"$0 [EOS]\",\n",
        "    special_tokens=[(\"[EOS]\", tokenizer.model.token_to_id(\"[EOS]\"))],\n",
        ")\n",
        "pretokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "pretokenizer.add_special_tokens({'pad_token': '[PAD]', 'cls_token': \"[CLS]\", 'eos_token': \"[EOS]\"})\n",
        "print(f\"Size of the dictionary: {len(pretokenizer)}\")\n",
        "print(tokenizer.encode(*generate_data()).tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBqZlOCVECnZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    model_inputs = pretokenizer(examples[\"text\"], text_target=examples[\"out\"])\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_data = dataset.map(preprocess_function, batched=True)\n",
        "print(tokenized_data[\"train\"][10])\n",
        "print(tokenized_data[\"train\"][11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMMdHvx1dAZt"
      },
      "source": [
        "## Create the Model\n",
        "\n",
        "There are three basic types of Transformers models:\n",
        "- Decoder-only: sequence is processed from left to right, each token sees only the previous ones, like GPT.\n",
        "- Encoder-only: i.e. BERT, sequence is processed without any order. This does not allow autoregressive inference, however is better for sequence classification, since each token can get information from any other one.\n",
        "- Encoder-Decoder, with two sequences, one input sequence processed using an Encoder, and an output sequence is generated sequentially using a Decoder that can also attend to the output of the Encoder.\n",
        "\n",
        "\n",
        "The general rule of thumb is to use Encoder-Decoder architectures for a Sequence to Sequence tasks, for example for a translation.\n",
        "\n",
        "\n",
        "Here, we're also having a Seq2Seq task, but we could also use a Decoder-only architecture. Why it might be a worse idea? Note that, for a `demo` task, Encoder-only architecture could also be utilized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX6fxOn2I7VS"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "\n",
        "args = dict(\n",
        "    vocab_size=len(pretokenizer),\n",
        "    hidden_size=256,\n",
        "    num_hidden_layers=4,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=1024  # 4*hidden_size is the standard\n",
        "    )\n",
        "config_encoder = BertConfig(**args)\n",
        "config_decoder = BertConfig(**args)\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
        "\n",
        "# rewrite special token id's to the model\n",
        "config.decoder_start_token_id = pretokenizer.cls_token_id\n",
        "config.pad_token_id = pretokenizer.pad_token_id\n",
        "config.eos_token_id = pretokenizer.eos_token_id\n",
        "config.unk_token_id = pretokenizer.unk_token_id\n",
        "\n",
        "# Initializing a Bert2Bert model (with random weights) from the bert-base-uncased style configurations\n",
        "model = EncoderDecoderModel(config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPA7K7FTv8SF"
      },
      "source": [
        "## Create a Hugging Face Repository\n",
        "Remember that Google Colab does not have any permanent storage and after you restart your session your model will be lost. Hugging Face provides a convenient way to store and share your models with others through repositories.\n",
        "\n",
        "\n",
        "To create a repository you first need to have an account: https://huggingface.co/join\n",
        "\n",
        "\n",
        "Then, create an HF authentication token: https://huggingface.co/settings/tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llVL2Wbyv58Z"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_repo\n",
        "import os\n",
        "\n",
        "repo_name = \"calculator_model_test\"\n",
        "# insert your token here\n",
        "access_token = \"SECRET\"\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = access_token\n",
        "create_repo(repo_name, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGFv-qcF4DJP"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "This might take a while if you do not use GPU acceleration.\n",
        "\n",
        "\n",
        "How is the training different from the `demo` version? In theory, the `step-by-step` calculation forces the model to produce complex output, which might be harder to learn. On the other hand, it also uses more computation to generate the output (that might be beneficial to performance) and can benefit from in-context learning. Is loss enough to compare these two models?\n",
        "\n",
        "\n",
        "Note that, training loss is averaged from the whole epoch, while validation loss is calculated after the epoch ends, therefore when the training curve is steep, especially at the beginning, training loss may exceed the validation loss. Moreover, label smoothing and data augmentation can also influence this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb__VU-5fMZ5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=pretokenizer, model=model)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"calculator_model_test\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=512,\n",
        "    per_device_eval_batch_size=512,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=40,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,  # you can change it to True for a faster training on GPU\n",
        "    push_to_hub=True,\n",
        "    hub_token=access_token,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=pretokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmtt2t5iYRSI"
      },
      "source": [
        "## Save the Model to HF\n",
        "\n",
        "We can easily save the trained model to the repository. This will create a new commit, with the model and the tokenizer stored as Git LFS files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHnNUoFcYVE1"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM62vsmol2lH"
      },
      "source": [
        "## Download the Model from HF\n",
        "\n",
        "Now we can download and try to evaluate the model. You can also download a model someone else has created by providing their username.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm8Ah0L5l6Ox"
      },
      "outputs": [],
      "source": [
        "from transformers import EncoderDecoderModel\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# insert your (or someone else's) username here\n",
        "login=\"ludziej\"\n",
        "\n",
        "pretokenizer = PreTrainedTokenizerFast.from_pretrained(f\"{login}/{repo_name}\")\n",
        "model = EncoderDecoderModel.from_pretrained(f\"{login}/{repo_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lYEjtXJCGa"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Note that, `model.generate` function will stop after the model returns the `[EOS]` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8aWxwpvjQqq"
      },
      "outputs": [],
      "source": [
        "def evaluate(text, skip_special_tokens=True):\n",
        "  inputs = pretokenizer(text, return_tensors=\"pt\").input_ids\n",
        "  inputs = inputs.to(model.device.type)\n",
        "  outputs = model.generate(inputs, max_new_tokens=10)\n",
        "  return pretokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
        "\n",
        "print(evaluate(\"3 + 2 - 2\", skip_special_tokens=False))\n",
        "print(evaluate(\"1 + 2 + 6\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RJ53iTzW4gP"
      },
      "source": [
        "## Gradio Inference API\n",
        "\n",
        "Using GradIO, we can easily create an interface that will allow us to test the app. This type of interface is available online while this cell is running and could also be embedded on the Hugging Face website with proper integration within the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qINM-XWLQ3dj"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "app = gr.Interface(\n",
        "    fn=evaluate,\n",
        "    inputs=[\"text\"],\n",
        "    outputs=[\"text\"],\n",
        "    description=\"Calculator\"\n",
        ")\n",
        "app.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7pkvunzxaxS"
      },
      "source": [
        "## Improve the Model\n",
        "\n",
        "Try to manually assess the model's performance using the app above. Where are the weak points? What may be a possible solution? You can try to use this notebook to fix identified problems. First, try to change training hyperparameters. What's the best validation accuracy you can get?\n",
        "\n",
        "You can also try to change the dataset to create some other calculators that i.e allow multiplication or a wider range of numbers.\n",
        "\n",
        "## Further reading\n",
        "\n",
        "The lecture inspired by Grokking Blogpost by Neel Nanda:  https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\n",
        "\n",
        "\n",
        "Grokking is a phenomenon, where neural networks at first learn to memorize the training dataset and then, within a phase change, learn the features that allow them to extrapolate to the validation dataset. This can be observed in a simple scenario of a 2-layer MLP trained on modular addition and was part of Mechanistic Intepretability research by Neel Nanda. However, this is an advanced reading, beyond the scope of the lecture, recommended to advanced students.\n",
        "This notebook may be an environment to experiment with similar ideas.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xaL50VdJLp5f",
        "aA6Oog8XLvrc",
        "Dezi2k7DAnsR",
        "h3V2-WE0Lotl",
        "LMMdHvx1dAZt",
        "JPA7K7FTv8SF",
        "BGFv-qcF4DJP",
        "gmtt2t5iYRSI",
        "cM62vsmol2lH",
        "d6lYEjtXJCGa",
        "2RJ53iTzW4gP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}