{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_VRo3GfIFp"
      },
      "source": [
        "From Switch Transformer paper:\n",
        "\n",
        ">In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost.\n",
        "\n",
        "A vanilla Transformer block looks like this:\n",
        "\n",
        "```python\n",
        "class ModernTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, up):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, n_heads)\n",
        "        self.mlp = nn.Sequential(\n",
        "            SwishGLU(embed_dim, embed_dim * up),\n",
        "            nn.Linear(embed_dim * up, embed_dim),\n",
        "        )\n",
        "        self.pre_attn_norm = RMSNorm(embed_dim)\n",
        "        self.pre_mlp_norm = RMSNorm(embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.pre_attn_norm(x))\n",
        "        x = x + self.mlp(self.pre_mlp_norm(x))\n",
        "        return x\n",
        "```\n",
        "\n",
        "The Mixture-of-Experts layer replaces the MLP layer. Instead of having one MLP layer, we have `num_experts` different MLP layers called *experts*.\n",
        "\n",
        "The idea is to process a contextualized token, by sending it to a subset of experts. In this way we could efficiently increase the number of parameters of the model without affecting computational cost too much.\n",
        "\n",
        "First, the token is fed into *router*, which determines to which experts a token should go to be processed. For computational reasons, there is a fixed limit on:\n",
        "* how many tokens an expert can process, and\n",
        "* by how many experts a token is processed.\n",
        "\n",
        "# Grading\n",
        "Your task is to implement a Mixture of Experts layer. You can get points for the following subtasks:\n",
        "1.  (5 points) Naive implementation of MoE layer that works with `num_experts_per_token>=1`\n",
        "2.  (5 points) Well-vectorized implementation of MoE layer that works with `num_experts_per_token=1`\n",
        "3.  (5 points) Implementation of a script testing for 1. 2. implementations output equivalence and performance superiority of 2.\n",
        "4.  (5 points) Well-vectorized implementation of MoE layer that works with `num_experts_per_token>=1`\n",
        "5.  (Bonus 5 points) Use Huggingface's Trainer class and compare performance of randomly initialized MoE Transformer and standard Transformer on `https://huggingface.co/datasets/imdb` dataset.\n",
        "\n",
        "20 points scored in this task is equivalent to at least 16% points achievable in this course.\n",
        "\n",
        "Please submit your assignments until 15th of April, 18:00 CET.\n",
        "\n",
        "# Rules\n",
        "- You shouldn't change basic `forward` and `initialization` signatures of the main classes: `Router` and `MoE`. You can add additional arguments with default values.\n",
        "- As an assignment, provide a Jupyter notebook with a short introduction at the top of what has been done and where.\n",
        "- You can add or remove any other classes, though you should keep the behaviour of `MLP` class somehow.\n",
        "- Sensible vectorization is good enough for the maximum amount of points. There is no need to optimize performance to the max, just show that you can identify opportunities for vectorization and you are able to implement complex vectorizations.\n",
        "- If in doubt, direct questions to either Jan Ludziejewski or Juliusz Straszy≈Ñski.\n",
        "- A notebook that is hard to grade (crashing, obfuscated) might be scored for 0 points.\n",
        "\n",
        "# Hints\n",
        "- First, write a naive implementation, vectorized operations might be hard to analyze for correctness.\n",
        "- You can make randomness deterministic by appropriate torch functions.\n",
        "- If you have a hard time fulfilling fair randomness for token discarding, you can try keeping the earlier tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh3a3zPwfIFt"
      },
      "outputs": [],
      "source": [
        "%pip install torch_tb_profiler einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrwQghs3fIFt"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from transformers import PretrainedConfig\n",
        "import torch.nn.functional as F\n",
        "from einops import einsum\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMG2OUC1fIFu"
      },
      "source": [
        "# Router\n",
        "The router is a module which assigns tokens to experts. It answers two questions:\n",
        "1. Which tokens should be assigned to which expert.\n",
        "2. How much weight should be assigned to each expert. The weight is determined by similarity between the token embedding and the expert embedding\n",
        "\n",
        "The following conditions must be satisfied:\n",
        "1. The routing weights must sum to 1 for each token and be non-negative\n",
        "2. A token should have exactly `num_experts_per_token` non-zero weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyie1JaVfIFu"
      },
      "outputs": [],
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, num_experts] - expert routing weights\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts = config.num_experts\n",
        "\n",
        "        self.expert_embeddings = nn.Parameter(torch.randn(self.num_experts, self.hidden_size))\n",
        "        torch.nn.init.kaiming_uniform_(self.expert_embeddings, nonlinearity='linear')\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJph0HYCfIFv"
      },
      "source": [
        "The MoE module is a module which wraps around a set of expert modules and a router module.\n",
        "\n",
        "It takes input embeddings and routes them to the experts.\n",
        "\n",
        "Each token is processed individually by a subset of experts.\n",
        "\n",
        "The output token embedding is a weighted sum of the expert outputs.\n",
        "\n",
        "The weights are determined by the router module.\n",
        "\n",
        "The subset of experts is determined by non-zero weights in the routing output.\n",
        "\n",
        "Additionally each expert might process at most `expert_capacity = ceil((batch_size * seq_len) / num_experts * capacity_factor)` tokens\n",
        "\n",
        "Superfluous tokens to be discarded by a particular expert should be selected uniformly at random.\n",
        "\n",
        "Discarding should be equivalent to setting the appropriate routing weight to 0, while other weights remain the same.\n",
        "\n",
        "This means that a token is processed by at most num_experts_per_token experts with a sum of weights of at most 1.\n",
        "\n",
        "Specifically, this could mean that a token is processed by 0 experts - in this case the resulting embedding should be a zero tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYWT1tWwfIFv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class MoE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        # You can change experts representation if you want\n",
        "        self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRxR0HafIFw"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_0dW5DsfIFw"
      },
      "outputs": [],
      "source": [
        "base_config = dict(\n",
        "    vocab_size=5000,\n",
        "    max_position_embeddings=256,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    hidden_size=128,\n",
        "    intermediate_size=512,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "standard_config = PretrainedConfig(\n",
        "    **base_config,\n",
        "    ff_cls=MLP\n",
        ")\n",
        "\n",
        "moe_config = PretrainedConfig(\n",
        "    **base_config,\n",
        "    num_experts=4,\n",
        "    capacity_factor=2.0,\n",
        "    num_experts_per_token=1,\n",
        "    ff_cls=MoE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abAV-rA3fIFy"
      },
      "source": [
        "# Basic Transformer-related classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXd1lstpfIFy"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(Embedding, self).__init__()\n",
        "    self.word_embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "    self.pos_embed = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_length = x.shape\n",
        "    device = x.device\n",
        "    positions = torch.arange(0, seq_length).expand(\n",
        "        batch_size, seq_length).to(device)\n",
        "    embedding = self.word_embed(x) + self.pos_embed(positions)\n",
        "    return self.dropout(embedding)\n",
        "\n",
        "\n",
        "class MHSelfAttention(nn.Module):\n",
        "    def __init__(self, config: PretrainedConfig):\n",
        "        super(MHSelfAttention, self).__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.qkv = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, seq_length, hidden_size = embeddings.size()\n",
        "\n",
        "        result = self.qkv(embeddings)\n",
        "        q, k, v = rearrange(result, 'b s (qkv nah hdsz) -> qkv b nah s hdsz', nah=self.num_attention_heads, qkv=3).unbind(0)\n",
        "\n",
        "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(hidden_size)\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        contextualized_layer = torch.matmul(attention_probs, v)\n",
        "\n",
        "        outputs = rearrange(contextualized_layer, 'b nah s hdsz -> b s (nah hdsz)')\n",
        "        return outputs\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MHSelfAttention(config)\n",
        "        self.norm1 = nn.LayerNorm(config.hidden_size)\n",
        "        self.norm2 = nn.LayerNorm(config.hidden_size)\n",
        "        self.intermediate = config.ff_cls(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  x + self.norm1(self.dropout(self.attention(x)))\n",
        "        x =  x + self.norm2(self.dropout(self.intermediate(x)))\n",
        "        return x\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embeddings = Embedding(config)\n",
        "        self.layer = nn.Sequential(*[TransformerBlock(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoding = self.layer(embedding_output)\n",
        "        pooled_encoding = encoding.mean(dim=1)\n",
        "        logits = self.classifier(pooled_encoding)\n",
        "        loss = F.cross_entropy(logits, labels) if labels is not None else None\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'logits': logits,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg4Jm-uFfIFy"
      },
      "source": [
        "# Tokenizer training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdqQL-HNfIFz"
      },
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from datasets import load_dataset\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train_from_iterator(\n",
        "    dataset['train']['text'],\n",
        "    vocab_size=base_config['vocab_size'],\n",
        "    special_tokens=[\"<s>\", \"</s>\", \"<pad>\"],\n",
        "    min_frequency=2\n",
        ")\n",
        "tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=base_config['max_position_embeddings'])\n",
        "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\", length=base_config['max_position_embeddings'])\n",
        "tokenizer.model_max_length = base_config['max_position_embeddings']\n",
        "tokenizer.pad_token = \"<pad>\"\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def tokenize(row):\n",
        "    return {\n",
        "        'input_ids': tokenizer.encode(row['text']).ids,\n",
        "    }\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}