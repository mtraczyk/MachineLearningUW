{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Recognition HW1 - Semantic segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The goal of this task is to finish the implementation of a semantic segmentation model based on the [BiFPN](https://arxiv.org/abs/1911.09070) approach. \n",
    "And test the capabilities of the model on [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/).\n",
    "\n",
    "To be more precise the model will input a batch of RGB images of shape `(BATCH, 3, IMAGE_HEIGHT, IMAGE_WIDTH)` and output a segmentation mask `m` of shape `(BATCH, 3, PREDICTION_HEIGHT, PREDICTION_WIDTH)`.  \n",
    "The mask will be such that \n",
    "* `m[:, 0]` equals 1, whether the element of the image corresponds to the pet\n",
    "* `m[:, 1]` equals 1, whether the element of the image corresponds to the background\n",
    "* `m[:, 2]` equals 1, whether the element of the image corresponds to the pet outline\n",
    "\n",
    "This notebook is divided into several sections:\n",
    "* Preparation - 0 points (everything is already implemented)\n",
    "  * Installation and imports of packages\n",
    "  * code for preparation of the dataset\n",
    "  * utils for visualization\n",
    "* Model implementation - 7 points (split among several subtasks)\n",
    "  * backbone preparation - 2 points (extracting features from the backbone and initial processing of them)\n",
    "  * BiFPN feature fusion - 3 points (implementation of BiFPN feature aggregation mechanism)\n",
    "  * segmentation head - 1 points \n",
    "  * network - 1 points (assembling the whole model)\n",
    "* Training and evaluation - 2 points\n",
    "  * metrics - 1 point (you only need to implement accuracy and IOU metrics)\n",
    "  * eval loop - 0 points (already implemented)\n",
    "  * train loop - 1 point\n",
    "* Inspection and Ablations - 3 points\n",
    "  * checking the importance of BiFPN\n",
    "\n",
    "\n",
    "## General information\n",
    "Please write the code within\n",
    "```python\n",
    "## TODO {\n",
    "\n",
    "## }\n",
    "```\n",
    "blocks.\n",
    "Please do not modify other parts of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation - 0 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A package for visualizing the architecture of the model\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchinfo\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We downscale images to\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "\n",
    "# We will make predictions in a smaller scale\n",
    "PREDICTION_HEIGHT = 64\n",
    "PREDICTION_WIDTH = 64\n",
    "\n",
    "\n",
    "# We want to match data distribution\n",
    "# of the backbone that we will introduce later\n",
    "image_net_mean = [0.485, 0.456, 0.406]\n",
    "image_net_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "input_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        torchvision.transforms.ToTensor(),  # our input is an image\n",
    "        torchvision.transforms.Normalize(image_net_mean, image_net_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(\n",
    "            (PREDICTION_HEIGHT, PREDICTION_WIDTH),\n",
    "            interpolation=torchvision.transforms.InterpolationMode.NEAREST,\n",
    "        ),\n",
    "        torchvision.transforms.PILToTensor(),  # gives uint8\n",
    "        # we shift values from {1, 2, 3} to {0, 1, 2}\n",
    "        torchvision.transforms.Lambda(lambda x: (x.squeeze(-3) - 1).type(torch.long)), \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for dataset storage\n",
    "DATA_PATH = \"~/torch_datasets/oxford\"\n",
    "\n",
    "TRAIN_DATASET = torchvision.datasets.OxfordIIITPet(\n",
    "    root=DATA_PATH,\n",
    "    split=\"trainval\",\n",
    "    download=True,\n",
    "    target_types=\"segmentation\",\n",
    "    transform=input_transforms,\n",
    "    target_transform=target_transforms,\n",
    ")\n",
    "TEST_DATASET = torchvision.datasets.OxfordIIITPet(\n",
    "    root=DATA_PATH,\n",
    "    split=\"test\",\n",
    "    download=True,\n",
    "    target_types=\"segmentation\",\n",
    "    transform=input_transforms,\n",
    "    target_transform=target_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "TRAIN_LOADER = torch.utils.data.DataLoader(\n",
    "    TRAIN_DATASET, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "TEST_LOADER = torch.utils.data.DataLoader(\n",
    "    TEST_DATASET, shuffle=True, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(TRAIN_LOADER))\n",
    "images, masks = samples\n",
    "\n",
    "assert len(images.shape) == 4\n",
    "assert images.shape[0] == BATCH_SIZE\n",
    "assert images.shape[1] == 3\n",
    "assert images.shape[2] == IMAGE_HEIGHT\n",
    "assert images.shape[3] == IMAGE_WIDTH\n",
    "assert images.dtype == torch.float32\n",
    "\n",
    "assert len(masks.shape) == 3\n",
    "assert masks.shape[0] == BATCH_SIZE\n",
    "assert masks.shape[1] == PREDICTION_HEIGHT\n",
    "assert masks.shape[2] == PREDICTION_WIDTH\n",
    "assert masks.dtype == torch.long\n",
    "assert (masks <= 2).all()\n",
    "assert (masks >= 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pet_mask(mask):\n",
    "    \"\"\"\n",
    "    Given a mask from Oxford-IIIT Pet Dataset after subtraction of 1\n",
    "    returns the array that encodes parts belonging to the pet.\n",
    "    \"\"\"\n",
    "    return (mask == 0).type(torch.long)\n",
    "\n",
    "\n",
    "def get_pet_background(mask):\n",
    "    \"\"\"\n",
    "    Given a mask from Oxford-IIIT Pet Dataset after subtraction of 1\n",
    "    returns the array that encodes parts belonging to the background.\n",
    "    \"\"\"\n",
    "    return (mask == 1).type(torch.long)\n",
    "\n",
    "\n",
    "def get_pet_outline(mask):\n",
    "    \"\"\"\n",
    "    Given a mask from Oxford-IIIT Pet Dataset after subtraction of 1\n",
    "    returns the array that encodes parts belonging to the pet outline.\n",
    "    \"\"\"\n",
    "    return (mask == 2).type(torch.long)\n",
    "\n",
    "\n",
    "def visualize_data(images, masks):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images: tensor of shape (BATCH, 3, H, W)\n",
    "        masks: tensor of shape (BATCH, H, W)\n",
    "    Draws a grid of images of size BATCH x 4.\n",
    "    I'th row consists of the image, pet mask, pet outline and background mask.\n",
    "    \"\"\"\n",
    "    assert len(images.shape) == 4\n",
    "    assert len(masks.shape) == 3\n",
    "    num_images = images.shape[0]\n",
    "    assert masks.shape[0] == num_images\n",
    "\n",
    "    pets, outlines, backgrounds = (\n",
    "        get_pet_mask(masks),\n",
    "        get_pet_outline(masks),\n",
    "        get_pet_background(masks),\n",
    "    )\n",
    "    _, axes = plt.subplots(num_images, 4, figsize=(32, 32))\n",
    "\n",
    "    for i, (img, p, o, b) in enumerate(zip(images, pets, outlines, backgrounds)):\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        axes[i, 0].imshow(np.clip(img.permute(1, 2, 0).numpy()/4.5 + 0.5, 0, 1))\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        axes[i, 1].imshow(p)  # foreground\n",
    "        axes[i, 2].axis(\"off\")\n",
    "        axes[i, 2].imshow(o)  # fore_background\n",
    "        axes[i, 3].axis(\"off\")\n",
    "        axes[i, 3].imshow(b)  # background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(images[:5], masks[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation - 7 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone preparation - 2 points\n",
    "Here, you will download the EfficientNet model and create classes that will allow you to extract features from the downloaded model. By features, we mean outputs of specified layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction - 1 point\n",
    "First, let's download and inspect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficient_net():\n",
    "    return torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "\n",
    "def get_example_input():\n",
    "    return torch.rand(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH) # [B, C, H, W]\n",
    "\n",
    "efficient_net = get_efficient_net()\n",
    "print(torchinfo.summary(efficient_net, input_size=(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)))\n",
    "print(efficient_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below according to the docstrings.  \n",
    "There are several approaches to the task below one of them is to use `register_forward_hook`.  \n",
    "The other one is to replace selected modules with appropriate wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNetFeatureExtractor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Accepts an efficient_net and an list of layers for feature extraction (feature_layer_ids).\n",
    "    When called returns a list containing the outputs of the selected layers.\n",
    "    The parameter feature_layer_ids specifies the ids of the layers from\n",
    "    efficient_net.layers.\n",
    "    To be more precise for feature_layer_ids = [4, 6] it returns a list containing outputs of \n",
    "    self.efficient_net.layers[4] and self.efficient_net.layers[6].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, efficient_net, feature_layer_ids: List[int]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.efficient_net = efficient_net\n",
    "        self.feature_layer_ids = feature_layer_ids\n",
    "\n",
    "        ## TODO {\n",
    "        \n",
    "        ## }\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "\n",
    "        ## TODO {\n",
    "        extracted_features = []\n",
    "        x = self.efficient_net.stem(x)\n",
    "        for i, layer in enumerate(self.efficient_net.layers):\n",
    "            x = layer(x)\n",
    "            if i in self.feature_layer_ids:\n",
    "                extracted_features.append(x)\n",
    "        ## }\n",
    "        return extracted_features\n",
    "\n",
    "\n",
    "def get_extracted_feature_shapes(net: EffNetFeatureExtractor):\n",
    "    extracted_features = net(get_example_input())\n",
    "    extracted_feature_shapes = [ef.shape for ef in extracted_features]\n",
    "    return extracted_feature_shapes\n",
    "\n",
    "\n",
    "##### TESTS START #####\n",
    "\n",
    "\n",
    "def test_feature_extraction():\n",
    "    efficient_net = get_efficient_net()\n",
    "    net = EffNetFeatureExtractor(\n",
    "        efficient_net=efficient_net, feature_layer_ids=[i for i in range(0, 7, 2)]\n",
    "    )\n",
    "    extracted_feature_shapes = get_extracted_feature_shapes(net)\n",
    "    extracted_feature_shapes = get_extracted_feature_shapes(net) # this is on purpose\n",
    "\n",
    "    expected_feature_shapes = [\n",
    "        (1, 16, 64, 64),\n",
    "        (1, 40, 16, 16),\n",
    "        (1, 112, 8, 8),\n",
    "        (1, 320, 4, 4),\n",
    "    ]\n",
    "    assert len(extracted_feature_shapes) == len(expected_feature_shapes)\n",
    "    for extracted_fs, expected_fs in zip(\n",
    "        extracted_feature_shapes, expected_feature_shapes\n",
    "    ):\n",
    "        assert extracted_fs == expected_fs\n",
    "\n",
    "\n",
    "test_feature_extraction()\n",
    "\n",
    "#####  TESTS END  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Channel Matching - 1 point\n",
    "In BiFPN we want all features to have the same number of channels (so we can add them easily).  \n",
    "Complete the code below according to docstrings.  \n",
    "`MatchChannels` is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "class MatchChannels(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Given the input of shape (BATCH, in_channels, H, W),\n",
    "    converts it to the one of shape (BATCH, out_channels, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1\n",
    "        )\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert len(x.shape) == 4\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BackboneWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Given a efficient net backbone and channel_matching_fn (a function of form lambda x: MatchChannels(x, c))\n",
    "    runs backbone to extract the features and uses modules created using channel_matching_fn to\n",
    "    make channel dimension equal in all features.\n",
    "    To be more specific\n",
    "    if the output of the backbone consists of features of the following shapes\n",
    "    (1, 16, 64, 64), (1, 18, 32, 32)\n",
    "    then BackboneWrapper makes them\n",
    "    (1, c, 64, 64), (1, c, 32, 32)\n",
    "\n",
    "    It also freezes the parameters of the backbone so that they do not receive gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone: EffNetFeatureExtractor, channel_matching_fn):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        ## TODO {\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.channel_matching_fn = channel_matching_fn\n",
    "        ## }\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "\n",
    "        ## TODO {\n",
    "        extracted_features = self.backbone(x)\n",
    "        \n",
    "        channel_matched_features = []\n",
    "        for ef in extracted_features:\n",
    "            cha_matching = self.channel_matching_fn(ef.shape[1])\n",
    "            unique_id = str(uuid.uuid1())\n",
    "            for name, param in cha_matching.named_parameters():\n",
    "                name = name.replace(\".\", \"_\") + \"_\" + unique_id\n",
    "                self.register_parameter(name, param)\n",
    "            channel_matched_features.append(cha_matching(ef))\n",
    "        ## }\n",
    "\n",
    "        return channel_matched_features\n",
    "\n",
    "\n",
    "##### TESTS START #####\n",
    "\n",
    "\n",
    "def test_backbone_basic():\n",
    "    efficient_net = get_efficient_net()\n",
    "    net = EffNetFeatureExtractor(\n",
    "        efficient_net=efficient_net, feature_layer_ids=[i for i in range(0, 7, 2)]\n",
    "    )\n",
    "\n",
    "    wrapped = BackboneWrapper(\n",
    "        backbone=net, channel_matching_fn=lambda x: MatchChannels(x, 24)\n",
    "    )\n",
    "\n",
    "    out = wrapped(get_example_input())\n",
    "\n",
    "    out_shapes = [feature.shape for feature in out]\n",
    "\n",
    "    expected_feature_shapes = [\n",
    "        (1, 24, 64, 64),\n",
    "        (1, 24, 16, 16),\n",
    "        (1, 24, 8, 8),\n",
    "        (1, 24, 4, 4),\n",
    "    ]\n",
    "    assert len(out_shapes) == len(expected_feature_shapes)\n",
    "    for extracted_fs, expected_fs in zip(out_shapes, expected_feature_shapes):\n",
    "        assert extracted_fs == expected_fs\n",
    "\n",
    "\n",
    "def test_backbone_gradients():\n",
    "\n",
    "    efficient_net = get_efficient_net()\n",
    "    net = EffNetFeatureExtractor(\n",
    "        efficient_net=efficient_net, feature_layer_ids=[i for i in range(0, 7, 2)]\n",
    "    )\n",
    "\n",
    "    wrapped = BackboneWrapper(\n",
    "        backbone=net, channel_matching_fn=lambda x: MatchChannels(x, 24)\n",
    "    )\n",
    "\n",
    "    out = wrapped(get_example_input())\n",
    "\n",
    "    sum = 0.0\n",
    "    for feature in out:\n",
    "        sum += feature.sum()\n",
    "\n",
    "    sum.backward()\n",
    "\n",
    "    backbone_parameter_names = set(\n",
    "        [f\"backbone.{name}\" for name, _ in wrapped.backbone.named_parameters()]\n",
    "    )\n",
    "    contains_trainable_param = False\n",
    "    for name, param in wrapped.named_parameters():\n",
    "        if name in backbone_parameter_names:\n",
    "            assert param.grad is None\n",
    "        else:\n",
    "            assert param.grad is not None\n",
    "            contains_trainable_param = True\n",
    "    assert contains_trainable_param\n",
    "\n",
    "\n",
    "test_backbone_basic()\n",
    "test_backbone_gradients()\n",
    "#####  TESTS END  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiFPN - Feature Fusion - 3 points\n",
    "Now we will implement the BiFPN network.  \n",
    "First, we will start with single-way feature fusion to emphasize similarities and differences with FPN.  \n",
    "Complete the code below according to docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiFPNSingle(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Given list of height features (list of length height),\n",
    "    with each feature having feature_channels channels\n",
    "    (that is i'th feature has shape (BATCH, feature_channels, H_i, W_i))\n",
    "    performs the following sequence of operations\n",
    "    res[0] = feature_list[0]\n",
    "    res[i] = ACT(BN(CONVS(w[i][1] * feature_list[i] + w[i][2]*RESIZE(res[i-1]) + w[i][3]*aux_feature_list[i])))\n",
    "    where the part w[i][3]*aux_feature_list[i] is present only when aux_feature_list is not None\n",
    "    and weights w are calculated like in the BiFPN paper.\n",
    "    Choice of CONVS is not specified.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_channels: int, height: int) -> None:\n",
    "        super().__init__()\n",
    "        ## TODO {\n",
    "        \n",
    "        ## }\n",
    "\n",
    "    def normalize_weights(self, weights: torch.Tensor):\n",
    "        \"\"\" \n",
    "        Makes weights \"add up to one\", uses either softmax or the\n",
    "        method from the BiFPN paper.\n",
    "        \"\"\"\n",
    "        ## TODO {\n",
    "        \n",
    "        ## }\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_list: List[torch.Tensor],\n",
    "        aux_feature_list: Optional[List[torch.Tensor]] = None,\n",
    "    ) -> List[torch.Tensor]:\n",
    "        ## TODO {\n",
    "        \n",
    "        ## }\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "##### TESTS START #####\n",
    "\n",
    "\n",
    "def test_bifpn_single():\n",
    "\n",
    "    bifpn_single = BiFPNSingle(32, 3)\n",
    "\n",
    "    input_data = [\n",
    "        torch.randn(1, 32, 64, 64),\n",
    "        torch.randn(1, 32, 32, 32),\n",
    "        torch.randn(1, 32, 7, 7),\n",
    "    ]\n",
    "\n",
    "    out = bifpn_single(input_data)\n",
    "\n",
    "    assert len(out) == len(input_data)\n",
    "    for o, i in zip(out, input_data):\n",
    "        assert o.shape == i.shape\n",
    "\n",
    "\n",
    "test_bifpn_single()\n",
    "#####  TESTS END  #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiFPN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Uses two BiFPNSingle modules to implement a BiFPN module as in the BiFPN paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_channels: int, height: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        ## TODO {\n",
    "\n",
    "        ## }\n",
    "\n",
    "    def forward(self, feature_list: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "\n",
    "        ## TODO {\n",
    "\n",
    "        ## }\n",
    "\n",
    "        return features_dec_spd\n",
    "\n",
    "\n",
    "##### TESTS START #####\n",
    "\n",
    "\n",
    "def test_bifpn():\n",
    "\n",
    "    bifpn_single = BiFPN(32, 3)\n",
    "\n",
    "    input_data = [\n",
    "        torch.randn(1, 32, 64, 64),\n",
    "        torch.randn(1, 32, 32, 32),\n",
    "        torch.randn(1, 32, 7, 7),\n",
    "    ]\n",
    "\n",
    "    out = bifpn_single(input_data)\n",
    "\n",
    "    assert len(out) == len(input_data)\n",
    "    for o, i in zip(out, input_data):\n",
    "        assert o.shape == i.shape\n",
    "\n",
    "\n",
    "test_bifpn_single()\n",
    "#####  TESTS END  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Head - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Given an input of shape (B, feature_channels, H, W)\n",
    "    Produces the output of shape (B, num_classes, H', W') (where H', W' = output_shape)\n",
    "    consisting of logits (that is no activation in the last layer) that can be used to classify each pixel.\n",
    "    To do so uses additional convolution(s) that operate on input with inner_channels channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_channels: int,\n",
    "        output_shape: Tuple[int, int] = (PREDICTION_HEIGHT, PREDICTION_WIDTH),\n",
    "        inner_channels: int = 64,\n",
    "        num_classes: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.num_classes = num_classes\n",
    "        ## TODO {\n",
    "\n",
    "        ## }\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## TODO {\n",
    "        \n",
    "        ## }\n",
    "\n",
    "        assert result.shape[0] == x.shape[0]\n",
    "        assert result.shape[1] == self.num_classes\n",
    "        assert result.shape[2:] == self.output_shape\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Uses BackboneWrapper,\n",
    "    BiFPN(s) and\n",
    "    SegmentationHead.\n",
    "    Given an input of shape (BATCH, 3, H, W)\n",
    "    returns a tensor of shape (BATCH, 3, H', W') (where H', W' = output_shape)\n",
    "    with logits for pet, background, and outline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_channels: int = 32,\n",
    "        output_shape: Tuple[int, int] = (PREDICTION_HEIGHT, PREDICTION_WIDTH),\n",
    "        feature_layer_ids: List[int] = [0, 1, 2, 3, 4, 5, 6],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        ## TODO {\n",
    "\n",
    "        ## }\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## TODO {\n",
    "        \n",
    "        ## }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation - 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics - 1 point\n",
    "\n",
    "Implement accuracy and IOU metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction: np.ndarray, ground_truth: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given prediction of shape (B, C, H, W)\n",
    "    and ground_truth of shape (B, C, H, W)\n",
    "    outputs an array of shape (B, C).\n",
    "    such that at the position (b, c) is the\n",
    "    fraction of all pixels correctly classified\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(prediction.shape) == 4\n",
    "    assert prediction.shape == ground_truth.shape\n",
    "    assert np.logical_or(prediction == 1, prediction == 0).all()\n",
    "    assert np.logical_or(ground_truth == 1, ground_truth == 0).all()\n",
    "\n",
    "    ## TODO {\n",
    "\n",
    "    ## }\n",
    "\n",
    "    assert result.shape == prediction.shape[:2]\n",
    "    return result\n",
    "\n",
    "\n",
    "##### TESTS START #####\n",
    "def test_accuracy():\n",
    "    a = np.zeros((1, 1, 64, 64))\n",
    "    a[:, :, 32:48, 32:48] = 1\n",
    "    b = np.zeros((1, 1, 64, 64))\n",
    "    b[:, :, 47:63, 47:63] = 1\n",
    "    assert np.isclose(\n",
    "        accuracy(np.concatenate([a, a], axis=1), np.concatenate([a, b], axis=1)),\n",
    "        np.array([1.0, (4096 - 510) / 4096])[None, :],\n",
    "    ).all()\n",
    "\n",
    "\n",
    "test_accuracy()\n",
    "#####  TESTS END  #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(prediction: np.ndarray, ground_truth: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given prediction of shape (B, C, H, W)\n",
    "    and ground_truth of shape (B, C, H, W)\n",
    "    outputs an array of shape (B, C).\n",
    "    such that at the position (b, c) is the\n",
    "    value of the intersection of the prediction and the ground_truth mask \n",
    "    (i.e. number of points where both are 1)\n",
    "    divided by their union (i.e. number of points where at least one is 1) \n",
    "    (assume 0/0 = 0).\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(prediction.shape) == 4\n",
    "    assert prediction.shape == ground_truth.shape\n",
    "    assert np.logical_or(prediction == 1, prediction == 0).all()\n",
    "    assert np.logical_or(ground_truth == 1, ground_truth == 0).all()\n",
    "    ## TODO {\n",
    "    \n",
    "    ## }\n",
    "    assert result.shape == prediction.shape[:2]\n",
    "    return result\n",
    "\n",
    "\n",
    "##### TESTS START #####\n",
    "\n",
    "def test_iou():\n",
    "\n",
    "    a = np.zeros((1, 1, 64, 64))\n",
    "    a[:, :, 32:48, 32:48] = 1\n",
    "    b = np.zeros((1, 1, 64, 64))\n",
    "    b[:, :, 47:63, 47:63] = 1\n",
    "\n",
    "    assert np.isclose(\n",
    "        iou_metric(np.concatenate([a, a], axis=1), np.concatenate([a, b], axis=1)),\n",
    "        np.array([1.0, 1.0 / 511.0])[None, :],\n",
    "    ).all()\n",
    "\n",
    "    c = np.zeros((1, 1, 64, 64))\n",
    "\n",
    "    assert np.isclose(\n",
    "        iou_metric(c, c),\n",
    "        np.array([0.0])[:, None],\n",
    "    ).all()\n",
    "\n",
    "\n",
    "test_iou()\n",
    "#####  TESTS END  #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Loop\n",
    "Eval loop is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "def one_hot_encode_prediction(predictions: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given predictions of shape (B, H, W)\n",
    "    with number\n",
    "    0 - representing pixels belonging to pet,\n",
    "    1 - background,\n",
    "    2 - outline\n",
    "    one hot encodes it as\n",
    "    single tensor of shape (B, 3, H, W), such that\n",
    "    element (b, c, h, w) is 1 if pixel corresponds\n",
    "    to class c and 0 otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(predictions.shape) == 3\n",
    "\n",
    "    predictions_oh = torch.nn.functional.one_hot(predictions, num_classes=3)\n",
    "    predictions_oh = predictions_oh.permute(0, 3, 1, 2)  # B, C, H, W\n",
    "\n",
    "    assert len(predictions_oh.shape) == 4\n",
    "    assert predictions_oh.shape[1] == 3\n",
    "\n",
    "    return predictions_oh\n",
    "\n",
    "@torch.no_grad\n",
    "def eval_fn(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    iou = []\n",
    "    acc = []\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        x, y = data\n",
    "        ground_truth = one_hot_encode_prediction(y).cpu().numpy()\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        logits = model(x)\n",
    "        prediction = torch.argmax(logits.detach(), dim=-3, keepdim=False)\n",
    "        prediction = one_hot_encode_prediction(prediction).cpu().numpy()\n",
    "\n",
    "        iou.append(np.sum(iou_metric(prediction, ground_truth), axis=0))\n",
    "        acc.append(np.sum(accuracy(prediction, ground_truth), axis=0))\n",
    "        total += prediction.shape[0]\n",
    "\n",
    "    iou = np.stack(iou, axis=-1).sum(-1) / total\n",
    "    acc = np.stack(acc, axis=-1).sum(-1) / total\n",
    "    assert len(iou.shape) == 1\n",
    "    assert len(acc.shape) == 1\n",
    "    assert iou.shape[0] == 3\n",
    "    assert acc.shape[0] == 3\n",
    "\n",
    "    print(f\"IOU PET: {iou[0]}\")\n",
    "    print(f\"IOU BG: {iou[1]}\")\n",
    "    print(f\"IOU OUT: {iou[2]}\")\n",
    "\n",
    "    print(f\"ACC PET: {acc[0]}\")\n",
    "    print(f\"ACC BG: {acc[1]}\")\n",
    "    print(f\"ACC OUT: {acc[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop - 1 point\n",
    "\n",
    "Complete the training loop.\n",
    "You can use `CrossEntropyLoss` as a loss. \n",
    "\n",
    "You should be able to achieve pet IOU >= 75%, background IOU >= 80%, and outline IOU >= 40% on the test set.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epoch,\n",
    "    eval_fn,\n",
    "    weight=torch.tensor([1.0, 1.0, 1.0], device=DEVICE),\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model - model that given an image outputs a tensor\n",
    "                with logits for determining whether a pixel belongs\n",
    "                to the pet, the background, or the pet outline.\n",
    "        eval_fn - function that given a model performs its evaluation on a given set;\n",
    "                  called at the end of every epoch to report both test and train performance\n",
    "        weight - used as weights for CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    ## TODO {\n",
    "    \n",
    "    ## }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer():\n",
    "    model = Net()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters())\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "train(model, optimizer, TRAIN_LOADER, TEST_LOADER, 3, eval_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection and Ablations - 3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's inspect the masks generated by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(TEST_LOADER))\n",
    "images, masks = samples\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(images.to(DEVICE))\n",
    "    pred = torch.argmax(pred, dim=-3).cpu()\n",
    "\n",
    "\n",
    "visualize_data(images[:5], pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Weights - 1p\n",
    "Inspect the weights learned by the first `BiFPNSingle` module of the first `BiFPN` in your model.  \n",
    "That is the `BiFPNSingle` module that is used after preparing the features with `BackboneWrapper`.  \n",
    "Make a plot visualizing them (the type of the plot is up to you).  \n",
    "Briefly describe the results.  \n",
    "Hint `.detach().cpu().numpy()` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO {\n",
    "## CODE\n",
    "\n",
    "## }\n",
    "\n",
    "## TODO {\n",
    "## Description\n",
    "\n",
    "\n",
    "## }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best and Worst Predictions - 1 point\n",
    "Visualize the best and worst predictions made by the model on the test set (according to IOU PET).\n",
    "Compare them with ground truths. Do you notice any problems? If you notice problems then how can they be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO {\n",
    "# CODE\n",
    "\n",
    "##}\n",
    "\n",
    "## TODO {\n",
    "# Answer \n",
    "\n",
    "## }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiFPN Importance - 1 point\n",
    "Check the importance of BiFPN feature fusion mechanism.  \n",
    "Make BiFPN identity in your model and check the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO {\n",
    "\n",
    "## }\n",
    "train(model, optimizer, TRAIN_LOADER, TEST_LOADER, 3, eval_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
