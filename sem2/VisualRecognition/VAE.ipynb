{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Variational AutoEncoder\n",
        "\n",
        "Task: Implement and train a probabilistic AutoEncoder called Variational AutoEncoder (VAE) on MNIST. A nice introduction to this topic is [here](https://www.jeremyjordan.me/variational-autoencoders/)."
      ],
      "metadata": {
        "id": "nnjs8SQNwQYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA2s1eIMuS2s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "x_dim  = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "\n",
        "lr = # TODO\n",
        "\n",
        "epochs = # TODO"
      ],
      "metadata": {
        "id": "XsxCNmlfuT9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'datasets'\n",
        "mnist_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) \n",
        "train_dataset = torchvision.datasets.MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
        "test_dataset  = torchvision.datasets.MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "metadata": {
        "id": "GlRmJAkDuZDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Encoder that consists of FC input_dim -> hidden_dim, FC hidden_dim -> hidden_dim, FC hidden_dim -> latent_dim\n",
        "# You can use LeakyReLU 0.2\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        # TODO ...\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # TODO ...\n",
        "        return # TODO"
      ],
      "metadata": {
        "id": "Ee1nFayFubAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Decoder that consists of FC latent_dim -> hidden_dim, FC hidden_dim -> hidden_dim, FC hidden_dim -> output_dim\n",
        "# You can use Sigmoid and LeakyReLU 0.2\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        # TODO ...\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # TODO ...\n",
        "        return # TODO"
      ],
      "metadata": {
        "id": "Qe6mbSECuc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "        \n",
        "    def reparameterization(self, mean, var):\n",
        "        # TODO: implement reparameterization\n",
        "        return # TODO\n",
        "                \n",
        "    def forward(self, x):\n",
        "        # TODO: implement forward function that uses reparameterization function\n",
        "        return # TODO"
      ],
      "metadata": {
        "id": "TJ0UJ_rwueeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "-HFXNwn4ugNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function for the Gaussian distribution prior is presented in https://arxiv.org/pdf/1907.08956.pdf, Eq. 43.\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    rec_loss = # TODO\n",
        "    KLD = # TODO\n",
        "    return rec_loss + KLD"
      ],
      "metadata": {
        "id": "VGAr6r06qUzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    overall_loss = 0\n",
        "    for i, (x, _) in enumerate(train_loader):\n",
        "        x = x.view(batch_size, x_dim)\n",
        "        x = x.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mean, log_var = model(x)\n",
        "        loss = loss_function(x, x_hat, mean, log_var)\n",
        "        \n",
        "        overall_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"Average Loss: \", overall_loss / (len(train_loader) * batch_size))"
      ],
      "metadata": {
        "id": "4N1LesRxuj0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (x, _) in enumerate(test_loader):\n",
        "        x = x.view(batch_size, x_dim)\n",
        "        x = x.to(device)\n",
        "        x_hat, _, _ = model(x)\n",
        "        break"
      ],
      "metadata": {
        "id": "2xJLMVqHunA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(x, n_samples=3):\n",
        "    x = x.view(batch_size, 28, 28) \n",
        "    for i in range(n_samples):\n",
        "      fig = plt.figure()\n",
        "      plt.imshow(x[i].cpu().numpy())"
      ],
      "metadata": {
        "id": "G9dIf6alupCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(x)"
      ],
      "metadata": {
        "id": "45ttudoOusFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(x_hat)"
      ],
      "metadata": {
        "id": "yJhbyHaisGD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: sample noise, generate new images from noise and show generted images"
      ],
      "metadata": {
        "id": "V54aShcYuuH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}