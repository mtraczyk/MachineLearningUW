{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Use PyTorch, GPU and Colab.\n",
    "\n",
    "Implement a simple convolutional neural network with training and validation on CIFAR10 dataset, limited to subset of 3k train and 3k test images.\n",
    "Train the neural network for ~30 epochs with Adam optimizer and batch size equal to 64. Implement validation on batches.\n",
    "  \n",
    "Code the following incrementally:\n",
    "- Prevent overfitting using a) dropout and b) data augmentation.\n",
    "- Add test augmentation.\n",
    "- For better results use transfer learning."
   ],
   "metadata": {
    "id": "N6OGNOjp44KD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1iS6jAG2Q8h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    " \n",
    "import random\n",
    "random.seed(0)\n",
    " \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose( [ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ] )\n",
    " \n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "path = '~/Downloads/CIFAR10/'\n",
    " \n",
    "def create_loader(path: str, train: bool, transform, download: bool = True) -> torch.utils.data.DataLoader:\n",
    "    dataset = torchvision.datasets.CIFAR10(root=path, train=train, download=download, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    " \n",
    "transform_tr =  transform\n",
    "transform_val = transform\n",
    " \n",
    "trainloader = create_loader(path, train=True, transform=transform_tr)\n",
    "testloader = create_loader(path, train=False, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# sample data\n",
    "display_limit = 8\n",
    " \n",
    "def imshow(img):\n",
    "  img = img / 2 + 0.5     # unnormalize\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "  plt.show()\n",
    " \n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    " \n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:display_limit]))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(display_limit)))"
   ],
   "metadata": {
    "id": "KtD77l8Q4B4h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "W, H = 32, 32\n",
    "Ch = 3\n",
    " \n",
    "# should have conv 32, conv 64, conv 128 (all with kernel size 3x3, stride 2 and padding 1) with relu activations, hidden linear layer 40 with relu, and linear output\n",
    "class Net(nn.Module): \n",
    "    def __init__(self, init_W, init_H, init_Ch):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=init_Ch,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(128 * init_W * init_H, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ],
   "metadata": {
    "id": "S_P4PZYV4Gv8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " # Use Adam optimizer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = Net()"
   ],
   "metadata": {
    "id": "ChBYMaU_4ZAf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    " \n",
    "epochs = 40\n",
    " \n",
    "def train(epochs, trainloader, net, log_interval):\n",
    "  writer = SummaryWriter()\n",
    "  optimizer = optim.Adam(net.parameters())\n",
    "  \n",
    "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))"
   ],
   "metadata": {
    "id": "qaa3omPF4a4r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_net(path, net):\n",
    "     torch.save(net.state_dict(), path)\n",
    " \n",
    "PATH = './cifar_net.pth'\n",
    "save_net(PATH, net)"
   ],
   "metadata": {
    "id": "DDvrgb_Z4h4I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_params(path, net):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "\n",
    "load_params(PATH, net)"
   ],
   "metadata": {
    "id": "KBIIOuYG4jpk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def validate(loader, name, net=net):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = net(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loader.dataset),\n",
    "        100. * correct / len(loader.dataset)))\n",
    " \n",
    "validate(trainloader, \"train\")\n",
    "validate(testloader, \"test\")"
   ],
   "metadata": {
    "id": "_LR9Rxx14mIW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "id": "ah9LYd284xSn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "id": "xrG5hVk74ykn"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
