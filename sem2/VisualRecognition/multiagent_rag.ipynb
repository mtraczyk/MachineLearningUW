{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoD9DDO6jgka"
      },
      "source": [
        "# Task\n",
        "Your task is to implement a framework of multi-agent collaboration that use RAG as a tool to answer a set of hard mathematical questions.\n",
        "\n",
        "You will be asked to build your pipeline using a variety of tools from open-source libraries as well as get hands-on experience with accelerating state-of-the-art models through quantization for inference on a free, commercial GPU.\n",
        "\n",
        "---------\n",
        "### Please note that you need to use a GPU instance to solve the exersize\n",
        "---------\n",
        "\n",
        "\n",
        "# Grading\n",
        "The subtasks as well as their respective points are given below:\n",
        "- Task 1 - Data preparation (2pt)\n",
        "- Task 2 - RAG preparation (3pt) / Custom Retriever (Extra 1pt)\n",
        "- Task 3 - ZS and RAG experiments (3pt)\n",
        "- Task 4 - Multi-Agent experiments (4pt)\n",
        "- Task 5 - Tutor Tool experiments (2pt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTnxmE50cAQZ"
      },
      "source": [
        "### Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwEtJvmtjmDC"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python -q\n",
        "!pip install llama-index -q\n",
        "!pip install numba -q\n",
        "!pip install llama-index-retrievers-bm25 -q\n",
        "!pip install datasets -q\n",
        "!pip install llama-index-vector-stores-postgres -q\n",
        "!pip install llama-index-embeddings-huggingface -q\n",
        "!pip install llama-index-llms-llama-cpp -q\n",
        "!pip install langchain-community -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g20ocbfncIUa"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brS1QM1ucJYY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import llama_index\n",
        "from llama_index.readers.file import CSVReader\n",
        "from llama_index.llms.llama_cpp import LlamaCPP as IndexWrapperLlama\n",
        "from langchain_community.llms import LlamaCpp as ChatWrapperLlama\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import PromptTemplate\n",
        "from tqdm.asyncio import tqdm\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from llama_index.core import QueryBundle\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcLbXEAtcDfW"
      },
      "source": [
        "### Task 1: Data & Model preparation\n",
        "##### You will work with the infamous [MATH dataset](https://github.com/hendrycks/math?tab=readme-ov-file#measuring-mathematical-problem-solving-with-the-math-dataset)\n",
        "\n",
        "It consists of two splits (train/test) and a total of 12500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"lighteval/MATH\") ### Use this link to download the dataset in Huggingface format."
      ],
      "metadata": {
        "id": "cw6G5Kve8Cql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In the cells below you need to:\n",
        "- Keep only rows that correspond to Level 1 difficulty (level column) and where the total length of the problem and the solution is no longer than 4028 characters. Keep only the problem and solution columns afterwards.\n",
        "- Preprocess the data by removing everything between [asy] and [/asy] that might exists in answers.\n",
        "- In the train set, merge the columns under a single one named ```problem_with_solution``` where the format is:\n",
        "```\n",
        "    Problem:\n",
        "    {content of problem column}\n",
        "\n",
        "    Solution:\n",
        "    {content of solution column}\n",
        "```\n",
        "- Save both dataframes as math_dataset_{```split```}.csv"
      ],
      "metadata": {
        "id": "84aidDcK8JfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3y2sri8qYh1"
      },
      "outputs": [],
      "source": [
        "# Access the training data\n",
        "train_data =\n",
        "\n",
        "# Access the test data\n",
        "test_data =\n",
        "\n",
        "def clean_problem(example):\n",
        "    \"\"\" Removes text between [asy] and [/asy] including the tags in the 'problem' column. \"\"\"\n",
        "    return\n",
        "\n",
        "# Apply the function to the 'problem' column\n",
        "train_data = train_data.map(clean_problem)\n",
        "# Apply the function to the 'solution' column and create a new column\n",
        "test_data = test_data.map(clean_problem)\n",
        "\n",
        "\n",
        "# Save them as CSVs #\n",
        "df = pd.DataFrame(train_data)\n",
        "df['problem_with_solution'] =\n",
        "df.to_csv(\"math_dataset_train.csv\", index=False)\n",
        "\n",
        "df = pd.DataFrame(test_data)\n",
        "df.to_csv(\"math_dataset_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPfbTiZglNr"
      },
      "source": [
        "#### In the following section you will start building the RAG pipeline using the Llamaindex library, Huggingface Embeddings and LlamaCPP inference acceleration framework.\n",
        "\n",
        "#### **Why Retrieval Augmented Generation?**\n",
        "In this exercise, you will leverage the strengths of in-context learning through a RAG approach to enhance your LLM's handling of complex mathematical problems. For each problem in the test set, RAG will retrieve relevant question-answer pairs from the training set to provide you with in-context examples. This method capitalizes on few-shot learning, enabling the LLM to quickly adapt without retraining, using these examples as direct references.\n",
        "\n",
        "This approach is especially beneficial in mathematics, where different problems require specific methods. By using RAG, you are supplied with pertinent, problem-specific data for each new query, enhancing your ability to solve diverse mathematical challenges effectively. This setup showcases how integrating RAG with few-shot principles can significantly boost performance by providing focused, relevant examples that guide you in real-time problem-solving.\n",
        "\n",
        "#### **To Implement the RAG pipeline fill the cells below to load 2 models:**\n",
        "- The Embedding model that will be used to:\n",
        " -  Convert the problem-answer pairs of our training set into a vector database.\n",
        "\n",
        " -  Match each incoming test problem with a number of retrieved items from the database.\n",
        "\n",
        "- The Chat model that given a query will:\n",
        "\n",
        "    - Reformulate and propose different versions of the query.\n",
        "\n",
        "    - Given the retrieved results from the vector database and the current problem, it will try to solve the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqDLMMXlpNbW"
      },
      "outputs": [],
      "source": [
        "### Embedding model setup ###\n",
        "### Hint: You can choose any library / model you want from API endpoints (OpenAI, Cohere ...), Sentence Transformers or HuggingFaceEmbeddings (preffered)\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXosZs-Ti6WK"
      },
      "source": [
        "##### For the model we will use the [LlamaCPP library](https://llama-cpp-python.readthedocs.io/en/stable/) which offers lightning-fast speed in inference on a commerical GPU.\n",
        "---------------------------------------------------------------------\n",
        "##### The suggested model to use will be the Quantized Version of the latest Mistral-7B Instruct model. Feel free to use any other model you want but be careful regarding GPU Memory requirements!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "9UmoRdsuLEgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k1YCMXxwDZO"
      },
      "outputs": [],
      "source": [
        "llm = IndexWrapperLlama(\n",
        "    model_url=None,\n",
        "    model_path='mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
        "    temperature= #???,\n",
        "    max_new_tokens= #???,\n",
        "    context_window=32000, ### Make sure it fits the GPU you use. Depending on your context and the K value at RAG (see below) you will need 10~12k. If possible use all 32k.\n",
        "    model_kwargs={\"n_gpu_layers\": 32}, ### Make sure it fits the GPU you use. The model has 32 layers so technically all of them should fit in a free tier T4 GPU.\n",
        ")\n",
        "llm.verbose=False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Provide a short answer here on why you chose the values of temperature / max_new_tokens / context_window you chose.**\n",
        "----------------------------------------------------------------------------\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Qep-xw_7AM59"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3c4BWHfAbnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKfrdcbHk2YF"
      },
      "source": [
        "#### Let's create now a VectoreStore Index using the joint problem-solution column of the training dataset.\n",
        "#### Use the training set csv to populate your DB.\n",
        "*(Hint: Use VectorStoreIndex class of LlamaIndex)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqpQit9fpMVb"
      },
      "outputs": [],
      "source": [
        "loader = CSVReader()\n",
        "documents = loader.load_data(file=Path('./math_dataset_train.csv'))\n",
        "\n",
        "splitter =\n",
        "index ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JAyTSQlNXl"
      },
      "source": [
        "#### Let's talk about Query Expansion, and why it is useful in complex RAG scenarios.\n",
        "-----------------------------------------------------------------------------\n",
        "Query expansion is a technique used to enhance the scope of a search query by generating additional related queries. This approach enriches the query process, increasing the likelihood of retrieving more comprehensive and relevant information. By generating variations of a math query, the system can pull from a wider array of similar past problems, leading to more robust and reliable answers.\n",
        "\n",
        "#### **To Implement Query Expansion fill the cells below to:**\n",
        "- Prompt your Chat Model with a query (test math problem) and return N different reformulations of the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILby0BPNyTFg"
      },
      "outputs": [],
      "source": [
        "query_gen_prompt_str = (\n",
        "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
        "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
        "    \"related to the following input query:\\n\"\n",
        "    \"Query: {query}\\n\"\n",
        "    \"Queries:\\n\"\n",
        ")\n",
        "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
        "\n",
        "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
        "    \"\"\"\n",
        "        Fill in the code to return a list of the original query (query_str) followed by (num_queries - 1) queries generated by the llm.\n",
        "    \"\"\"\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNByM36JlWIP"
      },
      "outputs": [],
      "source": [
        "### Test: It should return the original query plus 9 new queries ###\n",
        "query_str = \"What is the solution of 2^{x-3} = 3^{x-2}?\"\n",
        "queries = generate_queries(llm, query_str, num_queries=10)\n",
        "for f in queries:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPp9L37klvWA"
      },
      "source": [
        "#### Implementing the Fusion Retriever\n",
        "##### The BM25 retriever:\n",
        "--------------------------------------------------------------------------\n",
        "BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a family of scoring functions with slightly different components and parameters.\n",
        "Given a query \\( Q \\), containing keywords \\( q_1, ..., q_n \\), the BM25 score of a document \\( D \\) is:\n",
        "\n",
        "$$\n",
        "\\text{score}(D,Q) = \\sum _{i=1}^{n} \\text{IDF}(q_{i}) \\cdot \\frac{f(q_{i},D) \\cdot (k_{1}+1)}{f(q_{i},D) + k_{1} \\cdot (1-b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n",
        "$$\n",
        "\n",
        "where $f(q_{i}, D)$ is the number of times that the keyword $q_i$ occurs in the document $D$, $|D|$ is the length of the document $D$ in words, and avgdl is the average document length in the text collection from which documents are drawn. $k_1$ and $b$ are free parameters, usually chosen, in absence of an advanced optimization, as $k_1 \\in [1.2, 2.0]$ and $b = 0.75$.\n",
        "IDF($q_i$) is the inverse document frequency weight of the query term $q_i$.\n",
        "\n",
        "\n",
        "#### **To Implement Fusion Retriever fill the cells below to:**\n",
        "- Load the BM25 retriever from llama_index retrievers package\n",
        "- Initialize a Vector retriever from your created VectorStoreIndex class\n",
        "- Fill in the run_queries function below that will run a set of queries against your combined retrievers and return the top_k results.\n",
        "\n",
        "#### **For an extra 1 point:**\n",
        "- Extend the Retriever Class of LlamaIndex and create a custom BM25 retriever, with $k_1=1.5$ and $b = 0.7$  it does not have to be optimized, although a heap would be benefitial for performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyPcv5CI1a7f"
      },
      "outputs": [],
      "source": [
        "# Use common TOP_K in both retrievers\n",
        "TOP_K = 3\n",
        "NUMBER_OF_RETRIEVERS = 2\n",
        "## Vector retriever\n",
        "vector_retriever =\n",
        "\n",
        "## BM25 retriever\n",
        "bm25_retriever =\n",
        "\n",
        "async def run_queries(queries, retrievers):\n",
        "    \"\"\"Run queries against retrievers.\"\"\"\n",
        "    tasks = []\n",
        "    for query in queries:\n",
        "        for i, retriever in enumerate(retrievers):\n",
        "            tasks.append(retriever.aretrieve(query))\n",
        "\n",
        "    task_results = await tqdm.gather(*tasks)\n",
        "\n",
        "    results_dict = {}\n",
        "    for i, (query, query_result) in enumerate(zip([item for item in queries for _ in range(NUMBER_OF_RETRIEVERS)], task_results)):\n",
        "        results_dict[(query, i)] = query_result\n",
        "\n",
        "    return results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Important Note:\n",
        "\n",
        "--------------------------------------------\n",
        " Given N queries and M retrievers each with TOP_K results to return the ```run_queries``` function can theoretically return up to $N \\times M \\times TOP_K$ results. In reality, since a lot of the results are common between the retrievers, the total amount of retrieved items can be less.\n",
        "Expect the least amount of items to be $ N \\times TOP_K$, in the case where both retrievers would retrieve exactly the same things.\n",
        "\n",
        "#### The ```results_dict``` has N entries, each corresponding to a query given. #### Each item of ```results_dict``` is a ```List``` of ```Llamaindex Nodes``` #### with length ranging from $TOP_K$ (in case that the retrievers found the #### same items ) up to $M \\times TOP_K$ (in the case they found different items).\n",
        "---------------------------------------------------"
      ],
      "metadata": {
        "id": "MeqhOU08-NcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN1AJXP91dWn"
      },
      "outputs": [],
      "source": [
        "### Test ###\n",
        "query_str = \"What is the solution of 2^{x-3} = 3^{x-2}?\"\n",
        "queries = generate_queries(llm, query_str, num_queries=3)\n",
        "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])\n",
        "for f in results_dict:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68OHbOzbmDwG"
      },
      "source": [
        "#### **Retrieve and combine.**\n",
        "Fill the cell below so that your function collects the results from the combined retrieval - as Nodes in LlamaIndex - and sorts them according to their score (node.score), then it returns the top_k results based on that score.\n",
        "To get the actual text of a node use node.get_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sjj37N01fxo"
      },
      "outputs": [],
      "source": [
        "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
        "    \"\"\"\n",
        "    Create two dictionaries: fused_scores to store the cumulative scores for each unique text content, and text_to_node to map each text to its respective node and score.\n",
        "    Loop through a dictionary results_dict that contains lists of node objects with associated scores.\n",
        "        For each list of nodes:\n",
        "            Sort the nodes in descending order based on their scores.\n",
        "            Extract the text content of each node using a method like node.get_content().\n",
        "            For each text content, check if it's already in fused_scores. If not, initialize its score to 0.0.\n",
        "        Update the score of this text in fused_scores by adding the reciprocal of its rank (i.e., 1 divided by the position in the sorted list plus 1).\n",
        "    Sorting and Re-ranking:\n",
        "        Sort the fused_scores dictionary by value in descending order to prioritize texts with higher aggregated scores.\n",
        "    Adjusting Node Scores:\n",
        "        Based on the sorted scores, create a list ranked_nodes to store nodes with their updated scores.\n",
        "        Populate this list by mapping each text back to its original node, updating the node’s score to the newly calculated aggregated score.\n",
        "        Ensure only the top entries defined by similarity_top_k (an optional parameter with a default of 2) are returned.\n",
        "    Function Return:\n",
        "        The function should return the list of top nodes based on the sorted updated scores.\n",
        "    \"\"\"\n",
        "    fused_scores = {}\n",
        "    text_to_node = {}\n",
        "\n",
        "    # Compute scores\n",
        "    for nodes_with_scores in results_dict.values():\n",
        "        for rank, node_with_score in enumerate(\n",
        "            sorted(\n",
        "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
        "            )\n",
        "        ):\n",
        "\n",
        "    # Sort results\n",
        "    ranked_results =\n",
        "\n",
        "    # Adjust node scores\n",
        "    ranked_nodes: List[NodeWithScore] = []\n",
        "    for text, score in ranked_results.items():\n",
        "        # Fill this\n",
        "    return ranked_nodes[:similarity_top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttwGgaVbmR0s"
      },
      "outputs": [],
      "source": [
        "### Test it ###\n",
        "query_str = \"What is the solution of 2^{x-3} = 3^{x-2}?\"\n",
        "queries = generate_queries(llm, query_str, num_queries=4)\n",
        "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])\n",
        "final_results = fuse_results(results_dict)\n",
        "for n in final_results:\n",
        "    print(f\"Score: {n.score}\", \"\\n\", n.text, \"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mBnuXm4mzgP"
      },
      "source": [
        "#### **From Retrieval to Math problem solving:**\n",
        "Take a look at the FusionRetriever Class below:\n",
        "\n",
        "- It overrides the BaseRetriever class, by altering the functionality of the _retrieve function to implement query expansion given a provided argument.\n",
        "\n",
        "- Then it calls the fuse_results function to score the examples keeping the top_k ones, packing the previous steps in a single Retriever Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3tOHnGv1nhK"
      },
      "outputs": [],
      "source": [
        "class FusionRetriever(BaseRetriever):\n",
        "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        llmcpp,\n",
        "        retrievers: List[BaseRetriever],\n",
        "        similarity_top_k: int = 1,\n",
        "        n_query_expansion: int = 1,\n",
        "        enable_query_expansion: bool = False,\n",
        "\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        self._retrievers = retrievers\n",
        "        self._similarity_top_k = similarity_top_k\n",
        "        self._llmcpp = llmcpp\n",
        "        self.n_query_expansion = n_query_expansion\n",
        "        self.enable_query_expansion = enable_query_expansion\n",
        "        super().__init__()\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        \"\"\"Retrieve the results. REMEMBER: to run an async function use asyncio.run(###THE ASYNC FUNCTION###)\"\"\"\n",
        "        return final_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test the following function and answer the following questions:\n",
        "-  What do you observe with / without the query expansion flag given the query below?\n",
        "-  The retrieved text is simply added before our problem string. Given that our model is an Instruction Tuned model, is this correct?\n",
        "\n",
        "------------------------------------------------------------------------------\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "8sgUeOtSPyn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = \"\\nProblem:\\nWhat is the solution of 2^{x-3} = 3^{x-2}?\\nSolution:\\n\"\n",
        "fusion_retriever = FusionRetriever(\n",
        "   llm, [vector_retriever, bm25_retriever], similarity_top_k=3\n",
        ")\n",
        "query_engine = RetrieverQueryEngine.from_args(fusion_retriever, llm=llm)\n",
        "response = query_engine.query(query_str)\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "f-q97A0hPufn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Now, disable query expansion, set top_k to as high as your Gpu and context size enables (3 is a good value).\n",
        "\n",
        "In order to properly format the retrieved problem-solution pairs into useful in-context examples we need to prompt the Chat Model (through the RetrieverQueryEngine call).\n",
        "\n",
        "This can be done with the text_qa_template argument, which recieves a PromptTemplate class object. This object is a string that can be formated in 2 positions ```{context_str}``` and ```{query_str}```.\n",
        "\n",
        "For example:\n",
        "```\n",
        "text_qa_template_str = (\n",
        "    This is an example.\n",
        "    The retrieved items will be put here {context_str}.\n",
        "    While the current problem will be put here {query_str}.\n",
        ")\n",
        "text_qa_template = PromptTemplate(text_qa_template_str)\n",
        "\n",
        "```\n",
        "\n",
        "Your goal is to find an appropriate way to prompt your Chat Model so that it can properly utilize its context examples.\n"
      ],
      "metadata": {
        "id": "7bg23q4VPyZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkbNx-TPnDFS"
      },
      "outputs": [],
      "source": [
        "### Test it ###\n",
        "query_str = \"\\nProblem:\\nWhat is the solution of 2^{x-3} = 3^{x-2}?\\nSolution:\\n\"\n",
        "fusion_retriever = FusionRetriever(\n",
        "   llm, [vector_retriever, bm25_retriever], similarity_top_k=3\n",
        ")\n",
        "text_qa_template_str = ('FILL THIS')\n",
        "text_qa_template = PromptTemplate(text_qa_template_str)\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(fusion_retriever, llm=llm, text_qa_template=text_qa_template)\n",
        "response = query_engine.query(query_str)\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5yn3zzdC3G0"
      },
      "source": [
        "#### Iterate over the first **35** questions of the Test dataset and calculate the respective performance.\n",
        "\n",
        "#### **Perform the following check: Zero-Shot + Inception Prompting.**\n",
        "Prepend the problem task with an inception prompt (You are a...) which is found to be benefitial for performance in reasoning and math tasks.\n",
        "\n",
        "You will encounter the following  2 problems:\n",
        "- Your model will probably answer a long analytic solution with multiple steps and numbers here and there. Fill the extract_last_floating_number so that you get the last real (positive or negative) number from a string, and apply it to your solutions.\n",
        "- The MATH solutions are located at the end of the string enclosed in a \\boxed{} LaTeX command. Fill the extract_answer_from_boxed so that you get the ground truth answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_last_floating_number(text):\n",
        "    \"\"\"\n",
        "    Extracts the last real number from a string.\n",
        "    \"\"\"\n",
        "\n",
        "def extract_answer_from_boxed(expression):\n",
        "    \"\"\"\n",
        "    Extracts the content within the \\boxed{} LaTeX command.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "anNXzvtegjqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Iterate over 35 items of test set ###"
      ],
      "metadata": {
        "id": "kb8Geqel8E15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Iterate over the first **35** questions of the Test dataset and calculate the respective performance.\n",
        "\n",
        "#### **Perform the following check: FusionRAG + K-way.**\n",
        "Now test the FusionRAG performance by performing queries to your RetrieverQueryEngine made in the previous step.\n",
        "\n",
        "You need to fill the following infer_at_k function to perform the following things:\n",
        "- Accept as an argument the incoming math problem and a parameter k.\n",
        "- Retrieve the relevant in context examples from the FusionRetriever.\n",
        "- Format the retrieved incontext examples and pass them to the RetrieverQueryEngine.\n",
        "- Generate k responses from the RetrieverQueryEngine.\n",
        "---------------------------------------------------------------------\n",
        "- **Very Important:** Make sure you apply some post-processing to your responses.\n",
        "You most probably need to truncate them, if you see that the Chat Model after answering your question mades up another problem and tries to solve it as well.\n",
        "For example you might look for the keyword ```\"Problem:\"``` in the answer and keep everything before it.\n",
        "Same can be said for patterns like  multiple empty new lines ```'\\n\\n\\n'``` or ```'-----'```.\n",
        "- Your function should return two lists: The strings that represent the truncated responses and a list with the answers as numbers (or None if no number could be extracted). The lists should each have length equal to k.\n",
        "\n",
        "(Hint: Use the ```extract_last_floating_number``` to perform this)\n"
      ],
      "metadata": {
        "id": "QDpcVmKtgm-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_item(items):\n",
        "    if not items:\n",
        "        return 0\n",
        "    counter = Counter(items)\n",
        "    most_common, _ = counter.most_common(1)[0] if counter else (0, 0)\n",
        "    return most_common"
      ],
      "metadata": {
        "id": "rIie0IgyW5Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiFWvdsMC_Mr"
      },
      "outputs": [],
      "source": [
        "def infer_at_k(problem, k=3):\n",
        "    fusion_retriever =\n",
        "    text_qa_template_str = (\"FILL THIS\")\n",
        "    text_qa_template = PromptTemplate(text_qa_template_str)\n",
        "    query_engine =\n",
        "    responses = []\n",
        "    full_text_responses = []\n",
        "    return responses, full_text_responses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Iterate over the 35 test examples below ###\n",
        "-----------------------------------------------------------------------------\n",
        "Hint: It is quite useful - although not necessary - if you also save the generated responses and the generated numbers for each of the 35 questions into two pickle files named ```rag_test_responses.pt``` and ```rag_test_numbers.pt```"
      ],
      "metadata": {
        "id": "wTkhE9MXYZjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2eFhXESs7-en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer the following questions:\n",
        "- What would be an appropriate temperature value for trying multiple times to come up with an answer for the same question?\n",
        "- There are questions where the model answers correctly in all of the tries, and others where the model might answer correctly only once among its different tries. What does this phenomenon tell us about the math capabilities of the tested model?\n",
        "----------------------------------------------------------------------------\n",
        "Answer:"
      ],
      "metadata": {
        "id": "11zDPm7DZcKw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mw_jLaYnPaG"
      },
      "source": [
        "### Multi-Agent Setup with CrewAI\n",
        "\n",
        "From a single LLM to multiple agents: You might have heard about different ways that the concept of an agent has been incorporated into LLM communities. The benefits of delegating tasks and personalisation of Agents provide significant boosts over a single LLM model.\n",
        "\n",
        "Here you will use the [CrewAI library](https://docs.crewai.com/), to build a multi-agent system that will incorporating meta-cognition and error checking.\n",
        "\n",
        "-----------------------------------------------------------------------------\n",
        "Take a look at the documentation and examples of how to initialize an [Agent](https://docs.crewai.com/core-concepts/Agents/), a [Task](https://docs.crewai.com/core-concepts/Tasks/), a [Tool](https://docs.crewai.com/core-concepts/Tools/#creating-your-own-tools) and a [Crew](https://docs.crewai.com/core-concepts/Crews/).\n",
        "\n",
        "These are the only components you need!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgontj3FQO0q"
      },
      "outputs": [],
      "source": [
        "### Import Crew AI ###\n",
        "!pip install 'crewai[tools]' -q\n",
        "!pip install cohere -q\n",
        "!pip install anthropic -q\n",
        "!pip install -U langchain-anthropic\n",
        "!pip install --upgrade langchain_experimental -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from crewai import Crew, Process, Agent, Task\n",
        "from langchain_community.chat_models import ChatCohere\n",
        "from crewai_tools import BaseTool"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Our Goal now is to create the following multi-agent setup:\n",
        "\n",
        "RAG --> Chat Model Agent --> K-Answers --> Solution Analyzer --> Feedback --> Summary Writer --> Final Answer\n",
        "\n",
        "Here:\n",
        "* Solution Analyzer: Agent whose purpose is to look at the current problem and different ways to solve it, and choose the correct one out of them.\n",
        "If there are no suggested ways to solve the problem, or the agent thinks that none of them is correct, the agent can suggest its own solution.\n",
        "\n",
        "* Feedback: A string reflecting the response of the Solution Analyzer. Can be a step of solutions, a string saying \"All the steps are correct\" or anything else the Agent might respond.\n",
        "\n",
        "* Summary Writer: Agent whose purpose is to look through the Feedback and decide what is the correct number value to be extracted from it as a solution. It will emulate the behaviour of the ```extract_last_floating_number``` function you created above but in amore context-aware manner.\n",
        "\n",
        "* Final Answer: A string / float representing the best out of k possible ways of answering the math question according to the agent pipeline.\n",
        "---------------------------------------------------------------------------\n",
        "**Each Agent has access only to the output of the previous Agent, and the inputs of the current Task.**\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "##### **Important:** Since we have already performed the step up to K-answers (and possibly saved the results) there is no reason of redoing it. If thats the way you want to proceed look at the ```retrieve_at_k``` function below and modify it so that it does not make calls to the ChatModel, but rather accesses the saved pickles instead.\n",
        "\n",
        "\n",
        "##### If you want to proceed with the retrieve_at_k function, there is no problem at all, you will just have to wait a bit more while evaluating the results.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "#### Now let's have a look at the following modified FusionRAG K-Way function:\n",
        "It performs the same as previously, but now it drops None in extracted number solutions (and their respective analytical solutions) and returns a context describing our models various attempts into solving the problem, if no proposed solution was made by our ChatModel it asks for help.\n"
      ],
      "metadata": {
        "id": "Umg4ataohc88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBY48StFL0B0"
      },
      "outputs": [],
      "source": [
        "def retrieve_at_k(argument, k=3):\n",
        "    message_of_solution = ''\n",
        "    message_of_no_solution = 'Unfortunately I have no idea how to solve this problem. Can you help me?'\n",
        "    ### Filter out non-None responses ###\n",
        "    numerical_responses, full_answers = infer_at_k(argument, k=k)\n",
        "\n",
        "    final_responses =  []\n",
        "    for n,f in zip(numerical_responses, full_answers):\n",
        "        if n is None:\n",
        "            pass\n",
        "        else:\n",
        "            final_responses.append(f)\n",
        "\n",
        "    if len(final_responses) == 0:\n",
        "        return message_of_no_solution\n",
        "    else:\n",
        "        for i in range(len(final_responses)):\n",
        "            message_of_solution += f'\\nSolution {i}:\\n{final_responses[i]}\\n'\n",
        "    return message_of_solution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Test it with a random problem ###\n"
      ],
      "metadata": {
        "id": "66c_SDKlhSGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIa5hv9XBn45"
      },
      "source": [
        "#### Create a MathTask class that implements as methods a set of custom tasks to be run by our agents.\n",
        "\n",
        "Tasks:\n",
        "* validation: The task of the Solution Analyzer. It needs to have access to the agent, the current task and the proposed solutions.\n",
        "\n",
        "* summary: The task of the Summary Writer. It needs access only to the current agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT9FdBncTVTY"
      },
      "outputs": [],
      "source": [
        "from crewai import Task\n",
        "from textwrap import dedent\n",
        "\n",
        "class MathTasks():\n",
        "  def validation(self, ### Fill the arguments\n",
        "  ):\n",
        "    return Task(description=dedent(f\"\"\"\n",
        "        Fill this\n",
        "      \"\"\"),\n",
        "      agent=agent,\n",
        "      expected_output='Fill this'\n",
        "    )\n",
        "\n",
        "  def summary(self,  ### Fill the arguments\n",
        "              ):\n",
        "    return Task(description=dedent(f\"\"\"\n",
        "        Fill this\n",
        "      \"\"\"),\n",
        "      agent=agent,\n",
        "      expected_output='Fill this'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeK59DkfPecf"
      },
      "source": [
        "#### Make up your crew using **ONE** of the following:\n",
        "* OpenAI (I recommend GPT3.5 Turbo)\n",
        "* Cohere (I recommend command-r-plus)\n",
        "* Anthropic (I recommend any model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TloPPMQqPkAE"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_models import ChatCohere\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] =\n",
        "#os.environ[\"COHERE_API_KEY\"] =\n",
        "#os.environ[\"ANTHROPIC_API_KEY\"] =\n",
        "\n",
        "#agent_base_llm = ChatCohere(model='command-r-plus', temperature=0.2)\n",
        "#agent_base_llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo-1106\")\n",
        "#agent_base_llm = ChatAnthropic(temperature=0.2, model_name=\"claude-3-haiku-20240307\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **OR** Keep the same model but at a different wrapper using Langchain experimental.\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "**Important:** If you choose this option you will need to release the GPU memory from the previous loaded model, you can do this by hitting Runtime-->Restart Session, load every import again and instead of the previous model load this model.\n",
        "------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "_Y8yelm6Hzf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.chat_models import Llama2Chat\n",
        "\n",
        "llm = ChatWrapperLlama(\n",
        "    model_url=None,\n",
        "    model_path='mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
        "    temperature=###,\n",
        "    max_new_tokens=###,\n",
        "    n_ctx=###, (This is the context length but with different argument name in this wrapper)\n",
        "    n_gpu_layers= 32,\n",
        "    top_p=0.95\n",
        ")\n",
        "llm.verbose=False\n",
        "\n",
        "agent_base_llm = Llama2Chat(llm=llm)"
      ],
      "metadata": {
        "id": "otXd6VVIOGYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmnJzq4HN3Kv"
      },
      "outputs": [],
      "source": [
        "MAX_RPM_GLOBAL = 100\n",
        "N_AGENTS = 2\n",
        "\n",
        "solution_analyzer = Agent(\n",
        "  role=\n",
        "  goal=\n",
        "  backstory=\n",
        "  llm=agent_base_llm,\n",
        "  max_rpm=MAX_RPM_GLOBAL // N_AGENTS,\n",
        ")\n",
        "\n",
        "summary_writer = Agent(\n",
        "  role=\n",
        "  goal=\n",
        "  backstory=\n",
        "  llm=agent_base_llm,\n",
        "  max_rpm=MAX_RPM_GLOBAL // N_AGENTS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPnD-nKQguF"
      },
      "outputs": [],
      "source": [
        "def pipeline(current_task, k=3):\n",
        "    # Define the tasks in sequence\n",
        "    proposed_solutions = ### FILL THIS and use K here\n",
        "    analysis_task = MathTasks().validation(### FILL THIS)\n",
        "    writing_task = MathTasks().summary(### FILL THIS)\n",
        "\n",
        "    # Form the crew with a sequential process\n",
        "    report_crew = Crew(### FILL THIS)\n",
        "    # Execute tasks\n",
        "    res = report_crew.kickoff()\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1JAiFUzEQuL"
      },
      "source": [
        "### Measure Multi-Agent Performance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SklyFuZLOsKr"
      },
      "outputs": [],
      "source": [
        "### Write code that iterates over the first 35 questions of the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question: What is the performance difference? Did the validation agent help? Did you see any issues with the summary writer?\n",
        "-------------------------------------------------------------------------------\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "dp0QIGeflnvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporate a Tutoring Mechanism.\n",
        "Our pipeline will now look like this:\n",
        "\n",
        "RAG --> Chat Model Agent --> K-Answers --> Solution Analyzer --> Feedback --> Tutor--> Ground Truth Hint --> Solution Analyzer --> Updated Feedback --> Summary Writer --> Final Answer\n",
        "\n",
        "* Tutor: Agent that given the current task, the provided solutions and access to the ground truth answer (from ```'./math_dataset_test.csv'```) will provide the Solution Analyzer with hints regarding the correctness of their answer. Try to make the Agent not reveal the correct answer if you can (optional).\n",
        "\n",
        "* The Solution Analyzer will be then engaged again in a new task called reflect where they should reflect on the tutors hint and decide on a final answer.\n",
        "\n",
        "------------------------------------------------------------------------------\n",
        "**Important**: To implement access to the ground truth data, you need to:\n",
        "\n",
        "Implement a Class SoftTutorDB that uses the embedding model you have loaded, to encode the ground truth problems in the test CSV (problems only). Then given a new problem it needs to return the top 1 solution (it is guranteed to find a solution). Then use the provided TutorTool below and incorporate it into your pipeline.\n",
        "\n",
        "This option is not guranteed to work well if a non-API model has been chosen for the multi-agent pipeline. So you will be graded on basis of implementation and not actuall performance for this task\n"
      ],
      "metadata": {
        "id": "Us2krwqZ-JPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftTutorDB:\n",
        "    def __init__(self, file='./math_dataset_test.csv', problem_index='problem', solution_index='solution'):\n",
        "        self.db = pd.read_csv(file)\n",
        "        self.pi = problem_index\n",
        "        self.si = solution_index\n",
        "        self._encode_problems()\n",
        "\n",
        "    def _encode_problems(self):\n",
        "        self.keys =\n",
        "        self.values =\n",
        "\n",
        "    def get(self, query):\n",
        "\n",
        "\n",
        "\n",
        "class TutorTool(BaseTool):\n",
        "    name: str = \"Tutoring Tool\"\n",
        "    description: str = \"Given a math problem this tool returns the correct solution to it.\"\n",
        "    db: object = SoftTutorDB()\n",
        "    def _run(self, problem: str) -> str:\n",
        "        return self.db.get(problem)"
      ],
      "metadata": {
        "id": "ocfFKARsuaeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expand your MathTask class to implement 2 more tasks: tutoring and reflection.\n",
        "\n",
        "----------------------------------------------------------------------------\n",
        "What items does each task need access to? Remember that each Agent has access only to the output of the previous Agent, and the input of the Task.\n"
      ],
      "metadata": {
        "id": "Xn6fMupbpKyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MathTasks():\n",
        "    def validation()\n",
        "\n",
        "    def summary()\n",
        "\n",
        "    def tutoring()\n",
        "\n",
        "    def reflect()"
      ],
      "metadata": {
        "id": "nptMFRSn-O4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyst = Agent(\n",
        "    role=\n",
        "    goal=\n",
        "    backstory=,\n",
        "    tools = [],\n",
        "    llm=agent_base_llm,\n",
        "    max_rpm=1,\n",
        ")\n",
        "\n",
        "tutor = Agent(\n",
        "  role=\n",
        "  goal=\n",
        "  backstory=,\n",
        "  tools = [TutorTool()],\n",
        "  llm=agent_base_llm,\n",
        "  max_rpm=1,\n",
        ")\n",
        "\n",
        "writer = Agent(\n",
        "    role=\n",
        "    goal=\n",
        "    backstory=,\n",
        "    tools = [],\n",
        "    llm=agent_base_llm,\n",
        "    max_rpm=1,\n",
        ")"
      ],
      "metadata": {
        "id": "_6pw8zF_zEfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(current_task, k=3):\n",
        "    # Define the tasks in sequence\n",
        "    proposed_solutions = ### FILL THIS and use K here\n",
        "    analysis_task = MathTasks().validation(### FILL THIS)\n",
        "    writing_task = MathTasks().summary(### FILL THIS)\n",
        "\n",
        "    # Form the crew with a sequential process\n",
        "    report_crew = Crew(### FILL THIS)\n",
        "    # Execute tasks\n",
        "    res = report_crew.kickoff()\n",
        "    return res"
      ],
      "metadata": {
        "id": "I4l0CC_sx1td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_with_tutoring(current_task, k):\n",
        "    # Define the tasks in sequence\n",
        "    proposed_solutions = ### FILL THIS and use K here\n",
        "    analysis_task = MathTasks().validation(### FILL THIS)\n",
        "    tutoring_task = MathTasks().tutoring(### FILL THIS)\n",
        "    reflection_task = MathTasks().reflect((### FILL THIS, Hint: Use context=[analysis_task, tutoring_task] as an extra argument)\n",
        "    writing_task = MathTasks().summary(### FILL THIS)\n",
        "\n",
        "    # Form the crew with a sequential process\n",
        "    report_crew = Crew()\n",
        "    # Execute tasks\n",
        "    res = report_crew.kickoff()\n",
        "    return res"
      ],
      "metadata": {
        "id": "lvI5vXGhKVio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Write code that iterates over the first 35 questions of the test dataset."
      ],
      "metadata": {
        "id": "D8VLoV_XKhyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7IZdJQzJ6DQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}