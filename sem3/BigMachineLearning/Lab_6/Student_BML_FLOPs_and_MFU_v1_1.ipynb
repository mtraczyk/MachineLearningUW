{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab - Calculating Model FLOPs, Memory, Memory Throughput, MFU\n",
        "\n",
        "In this lab, you will practice calculating and estimating the performance of LLMs in various scenarios. We will focus on the original LLaMA 7B model.\n",
        "\n",
        "Please refer to lectures about [Transformers](https://docs.google.com/presentation/d/1AmfsaJNq5A5HeNxSg6oXrc1quBk5yubEKCAJzBGZa0Q/edit?usp=sharing) and [GPUs](https://docs.google.com/presentation/d/1iHmOeFeSBbeN9VWB_ELxNxdJWEN10icnx_mI0MgXEgo/edit?usp=sharing)."
      ],
      "metadata": {
        "id": "xtuFwyMLdwUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Specification\n",
        "\n",
        "Here you can find the technical specifications for the A100 GPU: https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\n",
        "\n",
        "Technical specifications for the H100: https://www.nvidia.com/en-us/data-center/h100/\n",
        "\n",
        "Note that we usually do not use \"sparsity\", so when looking up FLOPS, use the (lower) number of FLOPS without sparsity. For example, for the A100 and BFLOAT16, we use 312 TFLOPS, not 624 TFLOPS with sparsity. If the number without sparsity is not provided (as in the H100 spec), it is probably just half of the sparsity number (so half of the claimed 1,979 TFLOPS is approximately 989 TFLOPS). Unfortunately, due to marketing departments, tech specs are usually provided with the highest possible number, not the most useful one.\n",
        "\n",
        "# Assumptions\n",
        "\n",
        "In the entire notebook, let's assume we use 16-bit numbers (bfloat16) everywhere. Therefore, 1 float = 2 bytes."
      ],
      "metadata": {
        "id": "Fb1GsrKr9PJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "a100_flops = ???   # for 16bit\n",
        "a100_mem_bytes = ???\n",
        "a100_mem_floats = ???   # assuming 16bit\n",
        "a100_mem_bandwith_bytes = ???\n",
        "a100_mem_bandwith_floats = ???"
      ],
      "metadata": {
        "id": "Wh0f2cgx-zxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of Parameters and Activations\n",
        "Let's take a config of the original LLaMA, e.g., from [here](https://huggingface.co/huggyllama/llama-7b/blob/main/config.json).\n",
        "\n",
        "OPTIONAL: You can also take a config from Llama 3 8B, e.g., from [here](https://huggingface.co/unsloth/llama-3-8b/blob/main/config.json) or the official source on [this HuggingFace page](https://huggingface.co/meta-llama/Meta-Llama-3-8B/blob/main/config.json) (inaccessible without logging in and agreeing to the model license). However, note that Llama 3 uses grouped query attentionâ€”essentially, it has four times smaller key-value linear projections and four times smaller key-value cache (see [this blogpost](https://adithyask.medium.com/from-7b-to-8b-parameters-understanding-weight-matrix-changes-in-llama-transformer-models-31ea7ed5fd88) for step-by-step calculations).\n",
        "\n",
        "Let's start with an easy one: how many parameters do we have in this model? Will it fit on a single GPU?"
      ],
      "metadata": {
        "id": "Ac4sBgsU-dM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Original LLaMA\n",
        "n_layers = ???\n",
        "vocab_size = ???\n",
        "d_model = ???\n",
        "# d_ff is also called an intermediate size\n",
        "d_ff = ???\n",
        "n_heads = ???\n",
        "d_heads = d_model // n_heads\n",
        "\n",
        "# Llama 3 8B\n",
        "# n_layers = <same>\n",
        "# vocab_size = 128256  # Llama 2 had 32000\n",
        "# d_model = <same>\n",
        "# d_ff = 14336  # Llama 2 had 11008\n",
        "# n_heads = <same>\n",
        "# d_heads = d_model // n_heads"
      ],
      "metadata": {
        "id": "U_qEkoeKrczR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Tip: you don't need to count biases, LayerNorms, etc.\n",
        "\n",
        "emb_params = ???\n",
        "unemb_params = ???\n",
        "# The field below is provided on purpose\n",
        "ff_layer_params = n_layers * ((d_model * d_ff) * 3) # 2 if ReLU, 3 if GeLU\n",
        "att_layer_params = ???\n",
        "total_params = emb_params + unemb_params + ff_layer_params + att_layer_params\n",
        "print(f\"{total_params=}\")"
      ],
      "metadata": {
        "id": "dWl46KJ4-cI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During inference, the majority of non-model memory usage comes from the KV cache. What's the size of the KV cache per token in this model?"
      ],
      "metadata": {
        "id": "BtLLnzmWHPH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "kv_cache_per_token = ???\n",
        "print(f\"{kv_cache_per_token=}\")"
      ],
      "metadata": {
        "id": "saYLmCWfHNR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, we must either remember all the activations of the model (standard training without activation checkpointing) or the activation after each Feed-Forward and Attention layer (training with activation checkpointing).\n",
        "\n",
        "What is the total size of all activations per token, assuming no checkpointing?\n",
        "\n",
        "What is the total size of all activations per token, assuming checkpointing after each Feed-Forward and each Attention?\n",
        "\n",
        "You can assume chunking of the loss layer (do not count outputs of the Unembedding). This technique will be described in a future lecture."
      ],
      "metadata": {
        "id": "gSRhl17jHyi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# It's okay to get this number approximately right, not exactly; don't focus on constants.\n",
        "activations_per_token_no_checkpoints = ???\n",
        "activations_per_token_with_checkpoints = ???\n",
        "\n",
        "print(f\"{activations_per_token_no_checkpoints=}\")\n",
        "print(f\"{activations_per_token_with_checkpoints=}\")"
      ],
      "metadata": {
        "id": "-rmyXMjPHxha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Maximum Number of Processed Tokens\n",
        "\n",
        "Assuming an 80GB A100, what is the maximum possible context length for inference? What about for training, with or without activation checkpointing?\n",
        "\n",
        "What if we assume 40GB instead of 80GB?\n",
        "Can we even fit the model for training (with gradients and Adam state) in 40GB?\n",
        "\n",
        "Remember to subtract model-dependent memory (weights, gradients, Adam state...) from available memory."
      ],
      "metadata": {
        "id": "KXO3-uqOLtpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# inference calculations\n",
        "inference_tokens_80gb = ???\n",
        "inference_tokens_40gb = ???\n",
        "\n",
        "print(f\"{inference_tokens_80gb=}\")\n",
        "print(f\"{inference_tokens_40gb=}\")"
      ],
      "metadata": {
        "id": "VN6J3BmoGslS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# training calculations\n",
        "training_tokens_no_checkpoints_80gb = ???\n",
        "training_tokens_with_checkpoints_80gb = ???\n",
        "\n",
        "print(f\"{training_tokens_no_checkpoints_80gb=}\")\n",
        "print(f\"{training_tokens_with_checkpoints_80gb=}\")\n",
        "\n",
        "training_tokens_no_checkpoints_40gb = ???\n",
        "training_tokens_with_checkpoints_40gb = ???\n",
        "\n",
        "print(f\"{training_tokens_no_checkpoints_40gb=}\")\n",
        "print(f\"{training_tokens_with_checkpoints_40gb=}\")"
      ],
      "metadata": {
        "id": "oc9YQ-BBHpEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLOPs to Train\n",
        "\n",
        "We will assume 100% MFU is possible if not bottlenecked by memory throughput (this is essentially impossible to achieve; usually, we can assume around 50%, depending on the model and load).\n",
        "\n",
        "We can assume the Attention mechanism doesn't take any computation (e.g., we have short sequences).\n",
        "\n",
        "For costs, we can assume a single hour of A100 is worth 2 USD."
      ],
      "metadata": {
        "id": "5IL9xGB2QoBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hint: if we don't care about memory transfer, batch size doesn't matter\n",
        "inference_flops_per_token = ???\n",
        "inference_time_per_token = ???\n",
        "print(f\"{inference_time_per_token=}\")\n",
        "\n",
        "training_no_checkpoints_flops_per_token = ???\n",
        "training_no_checkpoints_time_per_token = ???\n",
        "print(f\"{training_no_checkpoints_time_per_token=}\")\n",
        "\n",
        "training_with_checkpoints_flops_per_token = ???\n",
        "training_with_checkpoints_time_per_token = ???\n",
        "print(f\"{training_with_checkpoints_time_per_token=}\")"
      ],
      "metadata": {
        "id": "hyDpYz-VQm7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume you want to evaluate LLaMA on a dataset of 100M tokens. How much time do you need? What is the renting cost?\n",
        "\n",
        "(For now, let's assume 100% MFU and no issues with memory throughput.)"
      ],
      "metadata": {
        "id": "oHtzOf2fLuZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "gpu_hours = ???\n",
        "cost_USD = ???\n",
        "print(f\"{gpu_hours=}\")\n",
        "print(f\"{cost_USD=}\")"
      ],
      "metadata": {
        "id": "zbU-4F_cLt2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume you want to fine-tune LLaMA on a private dataset of 3B tokens. How much will it cost?\n",
        "\n",
        "(For now, let's assume 100% MFU and no problems with memory throughput.)"
      ],
      "metadata": {
        "id": "4r9cNpL2Mxze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "gpu_hours = ???\n",
        "cost_USD = ???\n",
        "print(f\"{gpu_hours=}\")\n",
        "print(f\"{cost_USD=}\")"
      ],
      "metadata": {
        "id": "idlYFZXHMu5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama was trained using approximately 1.4 trillion tokens. How many GPU hours would that be, assuming 100% FLOPS utilization?"
      ],
      "metadata": {
        "id": "KnJqYAGtKT63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "gpu_hours = ???\n",
        "cost_USD = ???\n",
        "print(f\"{gpu_hours=}\")\n",
        "print(f\"{cost_USD=}\")"
      ],
      "metadata": {
        "id": "bffv8s9gKJLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Throughput for Training\n",
        "\n",
        "Memory throughput makes larger batch sizes more critical. During each evaluation or training step, we generally need to read and write all model weights (and gradients, and Adam state), and read and write all activations.\n",
        "\n",
        "Because the number of reads/writes for model-type tensors is constant (independent of batch size), and the number of reads/writes for activation-type tensors grows linearly with batch size, larger batch sizes will generally be better.\n",
        "\n",
        "How long does it take to run an evaluation batch on 1 or 32 or 1024 or 32\\*1024 or 1024\\*1024 tokens? When are we bottlenecked by FLOPS, and when are we bottlenecked by memory throughput?"
      ],
      "metadata": {
        "id": "fBy206ZTKIHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# memory: doesn't matter how big is the batch size!\n",
        "# note that each activation must be both saved and loaded\n",
        "def get_transferring_time_per_token(batch_size, params, activations):\n",
        "  transfers_per_batch = ???\n",
        "  transfers_time_per_batch = ???\n",
        "  transfers_time_per_token = ???\n",
        "  return transfers_time_per_token\n",
        "\n",
        "for batch_size in [1, 32, 1024, 32*1024, 1024*1024]:\n",
        "  transferring_time_per_token = get_transferring_time_per_token(batch_size, total_params, activations_per_token_no_checkpoints)\n",
        "  print(f\"{batch_size=}\")\n",
        "  print(f\"{transferring_time_per_token=}\")"
      ],
      "metadata": {
        "id": "T0JM7Sunjplh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Max MFU\n",
        "\n",
        "If we are bottlenecked by memory loads (i.e., memory transfer requires more time than FLOPS), the MFU must be below 100%.\n",
        "\n",
        "Plot the possible MFU against the training batch size. You can assume that memory transfer happen in parallel with computation (this isn't always the case, but it's alright to assume here)."
      ],
      "metadata": {
        "id": "kfxqfWOXnkLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO by students\n",
        "def mfu_calc(batch_size, params, activations):\n",
        "  computation_time_per_token = ???\n",
        "  transferring_time_per_token = get_transferring_time_per_token(batch_size, params, activations)\n",
        "\n",
        "  mfu = ???\n",
        "  return mfu\n",
        "\n",
        "batch_sizes =  [2**x for x in range(20)]\n",
        "mfu_values = [mfu_calc(batch_size, total_params, activations_per_token_no_checkpoints) for batch_size in batch_sizes]\n",
        "\n",
        "# TODO: plot\n"
      ],
      "metadata": {
        "id": "zWbGk02PniVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Exercises\n",
        "\n",
        "You can calculate the answers to the questions below, or try to estimate.\n",
        "\n",
        "1. During autoregressive inference, we process only the last token while keeping the KV-cache for all previous tokens in the sequence. This KV-cache has to be read in each step of the inference. What is the context length beyond which we will always be bottlenecked by memory throughput, no matter the total batch size?\n",
        "\n",
        "2. Assuming a constant depth-to-width ratio, what is the \"minimal model\" that is reasonable to train on an A100? That is, the smallest model not bottlenecked by memory (with any batch size); you can plot MFU and check at what point there is a drop-off.\n",
        "\n",
        "3. Assuming a constant depth-to-width ratio, what is the minimal batch size for a given model to achieve 100% MFU (in theory)?\n",
        "\n",
        "4. What is the maximum model size one can train using a single A100 GPU? What if we have a node of 8xA100? How does it change with activation checkpointing?"
      ],
      "metadata": {
        "id": "3ye2YjwIm2S9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}