{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuaOAbbRWMg"
      },
      "source": [
        "# Robot control Lab 3 - OpenCV poses and stereo\n",
        "\n",
        "Todays lab will focus on extracting 3D information from images. During creation of this scenario Google Colab runs OpenCV version [4.8.0](https://docs.opencv.org/4.8.0/index.html), if you have a different version you can change docs version to match. If you want more materials or different approach to what will be presented in this scenario, [these materials](https://docs.opencv.org/4.6.0/d9/db7/tutorial_py_table_of_contents_calib3d.html) should talk about similar things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANnkm5rdLkf"
      },
      "source": [
        "## imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUeNgRCZR0UB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "print(f\"OpenCV version is: {cv2.__version__}\")\n",
        "\n",
        "# In Colab we need to use:\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -rf rc-2023-24\n",
        "!git clone https://github.com/mim-uw/rc-2023-24.git\n",
        "%cd rc-2023-24/docs/lab3-public"
      ],
      "metadata": {
        "id": "Hbi3SMIOPBis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4E9OalZcCow"
      },
      "source": [
        "## Calibration parameters and undistortion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv-HGkdZSQBG"
      },
      "source": [
        "Today we will be working with a camera with given calibration parameters. This means someone else (maybe the manufacturer) performed the calibration procedure and supplied us with the results. As you probably remember from the lecture, the camera matrix, or calibration matrix looks like this:\n",
        "\n",
        "\\begin{align}\n",
        "\\left[\\begin{array}{ccc}\n",
        "f_{x} & 0 & c_{x}\\\\\n",
        "0 & f_{y} & c_{y}\\\\\n",
        "0 & 0 & 1\n",
        "\\end{array}\\right]\n",
        "\\end{align}\n",
        "\n",
        "You should also be familiar with radial and tangential distortions that can be represented with a vector of 5 numbers:\n",
        "\n",
        "\\begin{align}\n",
        "\\left(\\begin{array}{ccccc}\n",
        "k_{1} & k_{2} & p_{1} & p_{2} & k_{3}\\end{array}\\right)\n",
        "\\end{align}\n",
        "\n",
        "More on that [here](https://docs.opencv.org/4.1.2/dc/dbb/tutorial_py_calibration.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQZQJcTXs3-7"
      },
      "outputs": [],
      "source": [
        "camera_matrix = np.array([[528.86 ,   0.    , 641.865],\n",
        "                          [  0.   , 528.755 , 360.867],\n",
        "                          [  0.   ,   0.    ,   1.   ]])\n",
        "\n",
        "dist_coeffs = np.array([-0.0420881, 0.0110919, -0.00090298, -0.00013151, -0.00534522])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx27Io7mvtn-"
      },
      "source": [
        "Today asset pictures are **not** undistorted. We have calibration parameters, so this should not be a problem.\n",
        "\n",
        "To **undistort** the images we need undistorion maps. Note that if we compute undistortion maps for a camera on one image, we can use the same maps to undistort other images taken with the same camera. So we need to find the maps only once.\n",
        "\n",
        "We find the maps in two steps. First we derive a new camera matrix using [getOptimalNewCameraMatrix](https://docs.opencv.org/4.1.2/d9/d0c/group__calib3d.html#ga7a6c4e032c97f03ba747966e6ad862b1). Then, we use the obtained matrix to calculate undistortion maps. Check the code below to see how this can be done.  Try different values of alpha, like 0 and 1, and see how the results change. Choose the value you deem the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeMjfhi-wCkn"
      },
      "outputs": [],
      "source": [
        "img0 = cv2.imread(\"0.png\")\n",
        "\n",
        "# OpenCV order\n",
        "size = (img0.shape[1], img0.shape[0])\n",
        "\n",
        "# Calculate undistorted camera matrix\n",
        "alpha = 0.5\n",
        "rect_camera_matrix = cv2.getOptimalNewCameraMatrix(camera_matrix, dist_coeffs, size, alpha)[0]\n",
        "\n",
        "# Calculate undistortion maps\n",
        "map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, np.eye(3), rect_camera_matrix, size, cv2.CV_32FC1)\n",
        "\n",
        "# Use maps to undistort an image\n",
        "rect_img0 = cv2.remap(img0, map1, map2, cv2.INTER_LINEAR)\n",
        "\n",
        "# Show original and undistorted side by side\n",
        "cv2_imshow(cv2.hconcat([img0, rect_img0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZX1-P1_AlKt"
      },
      "source": [
        "## Single image, single marker pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9qEhEYUFUIz"
      },
      "source": [
        "### Quick recap, OpenCV names:\n",
        "\n",
        "Pose = postition + orientation\n",
        "\n",
        "Position is a 3D translation vector:\n",
        "$\\left[\\begin{array}{c}d_{x}\\\\d_{y}\\\\d_{z}\\end{array}\\right]$, OpenCV will call this `tvec`.\n",
        "\n",
        "Orientation can be expressed in many ways: rotation matrix, rotation vector, quaternion, euler angles (e.g. roll, pitch, yaw).\n",
        "\n",
        "OpenCV most often works with rotation vector\n",
        "$\\left[\\begin{array}{c}r_{x}\\\\r_{y}\\\\r_{z}\\end{array}\\right]$,\n",
        "and calls it `rvec`.\n",
        "\n",
        "OpenCV also provides [Rodrigues](https://docs.opencv.org/4.1.2/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac) to convert `rvec` to and from the rotation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUQdYw-jHodA"
      },
      "source": [
        "### Detecting the pose\n",
        "\n",
        "As you remember from the lecture using pinhole camera model we can map a point in 3D world coordinates to a point on the image plane. The other direction is not so easy â€” point on the image plane gives us a line (light ray) in 3D on which this point in real world was. Now, we can use 4 corners of a marker, because we know their geometry, i.e. they lie on a single plane and form a square with a known side length.\n",
        "The process looks like so: we take corners on the image and imagine 4 rays, then we fit a square in 3D so that each corner lies on a ray and side lengths match. This image can help you visualize the process:\n",
        "\n",
        "![Projecting points into 3D](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99B415455D9C2AE71F)\n",
        "[Image source](https://ballentain.tistory.com/40)\n",
        "\n",
        "We will use a function `my_estimatePoseSingleMarkers` that solves that for by utilizing `solvePnP` function from opencv. It takes corners, marker size, intrinsic matrix and array containing parameters of distortions.\n",
        "\n",
        "Let's see how to use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33N48DDfFEd9"
      },
      "outputs": [],
      "source": [
        "def my_estimatePoseSingleMarkers(corners, marker_size, mtx, distortion):\n",
        "    '''\n",
        "    This will estimate the rvec and tvec for each of the marker corners detected by:\n",
        "       corners, ids, rejectedImgPoints = detector.detectMarkers(image)\n",
        "    corners - is an array of detected corners for each detected marker in the image\n",
        "    marker_size - is the size of the detected markers\n",
        "    mtx - is the camera matrix\n",
        "    distortion - is the camera distortion matrix\n",
        "    RETURN list of rvecs, tvecs, and trash (so that it corresponds to the old estimatePoseSingleMarkers())\n",
        "\n",
        "    stolen from stackoverflow\n",
        "    '''\n",
        "    marker_points = np.array([[-marker_size / 2, marker_size / 2, 0],\n",
        "                              [marker_size / 2, marker_size / 2, 0],\n",
        "                              [marker_size / 2, -marker_size / 2, 0],\n",
        "                              [-marker_size / 2, -marker_size / 2, 0]], dtype=np.float32)\n",
        "    trash = []\n",
        "    rvecs = []\n",
        "    tvecs = []\n",
        "    for c in corners:\n",
        "        nada, R, t = cv2.solvePnP(marker_points, c, mtx, distortion, False, cv2.SOLVEPNP_IPPE_SQUARE)\n",
        "        rvecs.append(R)\n",
        "        tvecs.append(t)\n",
        "        trash.append(nada)\n",
        "    return rvecs, tvecs, trash\n",
        "\n",
        "# Aruco detector parameters\n",
        "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_APRILTAG_16h5)\n",
        "detectorParams = cv2.aruco.DetectorParameters()\n",
        "detectorParams.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_CONTOUR\n",
        "detector = cv2.aruco.ArucoDetector(dictionary, detectorParams)\n",
        "\n",
        "# Note that unit is not specified, we just need to stick to one (here meters)\n",
        "MARKER_SIDE = 0.168\n",
        "\n",
        "img1 = cv2.imread(\"1.png\")\n",
        "img1_draw = img1.copy()\n",
        "\n",
        "corners, ids, _ = detector.detectMarkers(img1)\n",
        "\n",
        "# inspect the image and draw detection\n",
        "cv2_imshow(img1_draw)\n",
        "cv2.aruco.drawDetectedMarkers(img1_draw, corners)\n",
        "cv2_imshow(img1_draw)\n",
        "\n",
        "# This takes multiple corners and calculates 3D pose\n",
        "rvecs, tvecs, _ = my_estimatePoseSingleMarkers(corners, MARKER_SIDE, camera_matrix, dist_coeffs)\n",
        "\n",
        "# inspect type and shape of rvecs and tvecs\n",
        "print(np.array(rvecs).shape)\n",
        "print(np.array(tvecs).shape)\n",
        "\n",
        "# We can draw a pose using OpenCV\n",
        "img1_draw = img1.copy()\n",
        "cv2.drawFrameAxes(img1_draw, camera_matrix, dist_coeffs, rvecs[0], tvecs[0], 0.1)\n",
        "cv2_imshow(img1_draw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3irzuAr0VWkY"
      },
      "source": [
        "### Detections and undistortion\n",
        "\n",
        "Notice that we did not undistort the image. Marker detection often runs just fine on distorted images, but we had to pass not only the camera matrix, but also distortion coeffs to every function that calculated things in 3D. This way we didn't have to explicitly undistort the whole image, which can be slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTjghuoqWhVg"
      },
      "source": [
        "### Your turn, but undistort first\n",
        "\n",
        "Now try to merge knowledge about undistortion and marker's pose detection and load image 2.png, undistort it, and then perform markers detection and pose calculation.\n",
        "\n",
        "**Do not recalculate** things we already have: undistortion maps, undistorted camera matrix, marker detector parameters, marker size.\n",
        "\n",
        "Note: remember that undistortion changes camera matrix and makes distortion coeffitiens zero, so you should use `rect_camera_matrix` after undistortion and you can just pass `0` as distCoeffs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zQsiu0nXepL"
      },
      "outputs": [],
      "source": [
        "# undistort the image:\n",
        "img2 = cv2.imread(\"2.png\")\n",
        "\n",
        "# OpenCV order\n",
        "size = (img2.shape[1], img2.shape[0])\n",
        "\n",
        "# Calculate undistorted camera matrix\n",
        "alpha = 0.5\n",
        "rect_camera_matrix = cv2.getOptimalNewCameraMatrix(camera_matrix, dist_coeffs, size, alpha)[0]\n",
        "\n",
        "# Calculate undistortion maps\n",
        "map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, np.eye(3), rect_camera_matrix, size, cv2.CV_32FC1)\n",
        "\n",
        "# Use maps to undistort an image\n",
        "rect_img2 = cv2.remap(img2, map1, map2, cv2.INTER_LINEAR)\n",
        "\n",
        "# Show original and undistorted side by side\n",
        "cv2_imshow(cv2.hconcat([img2, rect_img2]))\n",
        "\n",
        "# Calculate the pose\n",
        "rect_img2_draw = rect_img2.copy()\n",
        "\n",
        "corners, ids, _ = detector.detectMarkers(rect_img2)\n",
        "\n",
        "# inspect the image and draw detection\n",
        "cv2_imshow(rect_img2_draw)\n",
        "cv2.aruco.drawDetectedMarkers(rect_img2_draw, corners)\n",
        "cv2_imshow(rect_img2_draw)\n",
        "\n",
        "# This takes multiple corners and calculates 3D pose\n",
        "rvecs, tvecs, _ = my_estimatePoseSingleMarkers(corners, MARKER_SIDE, camera_matrix, None)\n",
        "\n",
        "# TODO: inspect type and shape of rvecs and tvecs\n",
        "print(np.array(rvecs).shape)\n",
        "print(np.array(tvecs).shape)\n",
        "\n",
        "# We can draw a pose using OpenCV\n",
        "for (r, t) in zip(rvecs, tvecs):\n",
        "  cv2.drawFrameAxes(rect_img2_draw, camera_matrix, None, r, t, 0.1)\n",
        "cv2_imshow(rect_img2_draw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvSn4JnVaiuq"
      },
      "source": [
        "### Drawing\n",
        "\n",
        "Just as in the last scenario we will draw 3D poses ourselves, but now in 3D!\n",
        "\n",
        "We will use [projectPoints](https://docs.opencv.org/4.1.2/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) function, which takes object pose as `rvec` and `tvec` and an array of object points. Object points are just additional translations in object's frame of reference. For example, if we have object point `[42, 0, 0]` and will call `projectPoints` with some `rvec` and `tvec`, then the function will chain transormations from camera frame of reference first by `tvec`, then `rvec`, then translate in object's frame of reference by 42 in X direction and finally project resulting point to the image plane coordinates. For convenience the function can take multiple object points at once. Inspect the demo below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhxnGzWY1OHE"
      },
      "outputs": [],
      "source": [
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "DARK_GREEN = (0, 127, 0)\n",
        "RED = (0, 0, 255)\n",
        "BLUE = (255, 0, 0)\n",
        "VIOLET = (255, 0, 255)\n",
        "CYAN = (255, 255, 0)\n",
        "YELLOW = (0, 255, 255)\n",
        "ORANGE = (0, 100, 255)\n",
        "\n",
        "\n",
        "objpts = np.array([[0, 0, 0], [-0.5, 0, 0], [0.25, 0.25, 0]]) * MARKER_SIDE\n",
        "\n",
        "imgpts = np.rint(cv2.projectPoints(objpts, rvecs[0], tvecs[0], rect_camera_matrix, 0,)[0]).astype(int)\n",
        "imgpts = imgpts.reshape((-1, 2))\n",
        "\n",
        "rect_img2_draw = rect_img2.copy()\n",
        "\n",
        "cv2.circle(rect_img2_draw, (imgpts[0][0], imgpts[0][1]), 5, GREEN, -1);\n",
        "cv2.circle(rect_img2_draw, (imgpts[1][0], imgpts[1][1]), 5, VIOLET, -1);\n",
        "cv2.circle(rect_img2_draw, (imgpts[2][0], imgpts[2][1]), 5, ORANGE, -1);\n",
        "\n",
        "cv2_imshow(rect_img2_draw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-X_zOoi5qly"
      },
      "source": [
        "### Your turn! Drawing cubes\n",
        "\n",
        "On the `rect_img2` try to calculate and draw points in a cube pattern, like so:\n",
        "\n",
        "![cubes](https://github.com/nomagiclab/lab6_assets/raw/master/cubes.png)\n",
        "\n",
        "\n",
        "You will definitely need an array of object points that designate 8 cube corners, `rvecs` and `tvecs` (use calculated ones), `projectPoints` function and some drawing functions.\n",
        "\n",
        "You might be interested in `drawContours` function (search the docs!), but note that it takes list of contours and a contour is a list of points. This function lets you fill the contour, like the blue square in the example above or draw only the outline, like the red square in the example above.\n",
        "\n",
        "Remember to pass coordines of `int` type to OpenCV drawing functions. Also, some of them, like `line`, expect coordinates as a tuples, not lists or arrays (`drawContours` does not care)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzyUBx6OcfU7"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9uUuog-FwA"
      },
      "source": [
        "## Stereo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCyEKU1IE3vb"
      },
      "source": [
        "Let's imagine we have two cameras in one package!\n",
        "\n",
        "![Zed 2](https://cdn.stereolabs.com/assets/images/zed-2/zed-2-front.jpg)\n",
        "\n",
        "Actually, we were using this camera all the time, but we were using only one (left) lens. Now it is time to use both to calculate 3D coordinates, this is called stereoscopic imaging, or stereo camera. First, lets take a look at images from both lenses. Do you see the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UzqMB3VEsfu"
      },
      "outputs": [],
      "source": [
        "img3_left = cv2.imread(\"3_left.png\")\n",
        "img3_right = cv2.imread(\"3_right.png\")\n",
        "cv2_imshow(cv2.hconcat([img3_left, img3_right]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtV1Bisy-IJu"
      },
      "source": [
        "Stereoscopic imaging allows us to calculate 3D coordinates without knowing anything about geometry of the object, or scene. We can even do this with a single point (which will be demonstrated soon). Stereo camera is an example of depth camera, because it can provide information about the depth of the scene.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwLdXnby_GLe"
      },
      "source": [
        "\n",
        "### Depth calculation\n",
        "\n",
        "![](https://uc402321e515612522aacc57fcd9.previews.dropboxusercontent.com/p/thumb/ACGUAFQh1vTRD3AsElWNhIWlphs7QVo1b5Q-VPnfxXjmBNVlbjNIfM0bTnUe81n8mdbaqeR32jfrmfjKXclWKis7eWI1l50_R6J2Ldb_zUK6OsQV9CwXIZegP_wbX3j2y8YDrkfvoFeNwM1z1-jayFxH_6_iv3ho_xfPK-lUlqj3wRERn_xgD-36-zl8nA2cEe674dnP3ykWBqwJFB8wfi7V_qI_BmogXQo1MWK9wCbmyOd_2wgPrAcHViW7A7lT5acrP-nHRwpvWJDw61ObP4EyTUxxn_m7aL_yLBQ6CukolITcoRpLELiC1Q8X61Upix22VkWYQGCt79OEmy7FJRS4ktSLjSH8LRucW4031dZ5j69_4LbbXsRT-0q7N00hqyQ/p.png)\n",
        "\n",
        "Quick explanation:\n",
        " - We want to estimate the distance from the blue point to the cameras (pinholes) - we are looking for z\n",
        " - there are two cameras with pinhole 1 and pinhole 2 respectively\n",
        " - image planes are right behind the pinholes at distance f (focal length) and the blue point is projected onto them in different locations\n",
        " - baseline is the distance between cameras\n",
        " - if x and x' mark positions of the observed blue point in the image planes, then we define the disparity as (x - x')\n",
        " - we can calculate the distance using disparity, baseline and f\n",
        "\n",
        "From similar triangles:\n",
        "\\begin{align}\n",
        "disparity=x-x\\prime=\\frac{Bf}{Z}\n",
        "\\end{align}\n",
        "\n",
        "### Mujoco and depth detection\n",
        "\n",
        "Go back to the initial lab scenario and use your knowledge about depth detection to solve the problem in the simulator:\n",
        "[link](https://mim-uw.github.io/rc-2023-24/lab3-public/lab3.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEB_esFuRg4V"
      },
      "source": [
        "## Dense stereo matching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1W-ggdYwLZP"
      },
      "source": [
        "\n",
        "Last thing that should be mentioned when talking about stereo should be dense stereo matching, i.e. it is possible to find the disparity not only for some known features like a corner of a marker, but for every point in the image. This is done by trying to match pixels or blocks of pixels from the left image in the right image at the same height, but shifted left.\n",
        "\n",
        "There are some caveats here. Matching algorithms are not always perfect and almost always require a lot of tuning and are very sensitive to parameters. As you can see below there is some noise in detected depth map.\n",
        "More on that [here](https://docs.opencv.org/4.8.0/dd/d53/tutorial_py_depthmap.html)\n",
        "\n",
        "![](https://docs.opencv.org/3.4/disparity_map.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-Y-dmtJG2lk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}