{"cells":[{"cell_type":"markdown","metadata":{"id":"DxaMTckGDGgc"},"source":["<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n","\n","AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n","<hr>\n","\n","<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n","\n","<center>\n","Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n","Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n","Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n","Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n","    </center>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6y4l5BmxTNNU","executionInfo":{"status":"ok","timestamp":1698917182462,"user_tz":-60,"elapsed":5612,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}}},"outputs":[],"source":["import random\n","import numpy as np\n","from torchvision import datasets, transforms"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iHhqeGLsHcYl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698917182850,"user_tz":-60,"elapsed":391,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}},"outputId":"8c26d5dc-91c4-4653-efd3-26b20dd95fa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-02 09:26:22--  https://s3.amazonaws.com/img-datasets/mnist.npz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.103.176, 52.217.119.16, 54.231.170.24, ...\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.103.176|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11490434 (11M) [application/octet-stream]\n","Saving to: ‘mnist.npz’\n","\n","mnist.npz           100%[===================>]  10.96M  40.0MB/s    in 0.3s    \n","\n","2023-11-02 09:26:22 (40.0 MB/s) - ‘mnist.npz’ saved [11490434/11490434]\n","\n"]}],"source":["!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uutaqUkuVAuF","executionInfo":{"status":"ok","timestamp":1698917281271,"user_tz":-60,"elapsed":603,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}}},"outputs":[],"source":["# Let's read the mnist dataset\n","\n","def load_mnist(path='mnist.npz'):\n","    with np.load(path) as f:\n","        x_train, _y_train = f['x_train'], f['y_train']\n","        x_test, _y_test = f['x_test'], f['y_test']\n","\n","    x_train = x_train.reshape(-1, 28 * 28) / 255.\n","    x_test = x_test.reshape(-1, 28 * 28) / 255.\n","\n","    y_train = np.zeros((_y_train.shape[0], 10))\n","    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n","\n","    y_test = np.zeros((_y_test.shape[0], 10))\n","    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","(x_train, y_train), (x_test, y_test) = load_mnist()"]},{"cell_type":"markdown","metadata":{"id":"T5PPE1ldTNNx"},"source":["## Exercise 1\n","\n","In this exercise your task is to fill in the gaps in this code by implementing the backpropagation algorithm\n","Once this is done, you can run the network on the MNIST example and see how it performs. Feel free to play with the parameters. Your model should achieve 90%+ accuracy after a few epochs.\n","\n","\n","## Exercise 2 (Optional)\n","\n","Implement a \"fully vectorized\" version, i.e. one using matrix operations instead of going over examples one by one within a minibatch.\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"OsCgwvfHTNN0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698870658328,"user_tz":-60,"elapsed":1413463,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}},"outputId":"68dcd89d-2fad-497c-8d9f-e390f6ac47c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.639\n","Epoch: 1, Accuracy: 0.7765\n","Epoch: 2, Accuracy: 0.7958\n","Epoch: 3, Accuracy: 0.8056\n","Epoch: 4, Accuracy: 0.8125\n","Epoch: 5, Accuracy: 0.89\n","Epoch: 6, Accuracy: 0.9032\n","Epoch: 7, Accuracy: 0.909\n","Epoch: 8, Accuracy: 0.9111\n","Epoch: 9, Accuracy: 0.9133\n","Epoch: 10, Accuracy: 0.9154\n","Epoch: 11, Accuracy: 0.9177\n","Epoch: 12, Accuracy: 0.9199\n","Epoch: 13, Accuracy: 0.9209\n","Epoch: 14, Accuracy: 0.9222\n","Epoch: 15, Accuracy: 0.9231\n","Epoch: 16, Accuracy: 0.9239\n","Epoch: 17, Accuracy: 0.9243\n","Epoch: 18, Accuracy: 0.9253\n","Epoch: 19, Accuracy: 0.9261\n","Epoch: 20, Accuracy: 0.9266\n","Epoch: 21, Accuracy: 0.9267\n","Epoch: 22, Accuracy: 0.9274\n","Epoch: 23, Accuracy: 0.9278\n","Epoch: 24, Accuracy: 0.9284\n","Epoch: 25, Accuracy: 0.9282\n","Epoch: 26, Accuracy: 0.929\n","Epoch: 27, Accuracy: 0.9296\n","Epoch: 28, Accuracy: 0.9302\n","Epoch: 29, Accuracy: 0.9302\n","Epoch: 30, Accuracy: 0.9307\n","Epoch: 31, Accuracy: 0.9314\n","Epoch: 32, Accuracy: 0.9316\n","Epoch: 33, Accuracy: 0.9317\n","Epoch: 34, Accuracy: 0.9318\n","Epoch: 35, Accuracy: 0.9325\n","Epoch: 36, Accuracy: 0.9325\n","Epoch: 37, Accuracy: 0.9327\n","Epoch: 38, Accuracy: 0.9324\n","Epoch: 39, Accuracy: 0.9328\n","Epoch: 40, Accuracy: 0.933\n","Epoch: 41, Accuracy: 0.9336\n","Epoch: 42, Accuracy: 0.934\n","Epoch: 43, Accuracy: 0.9342\n","Epoch: 44, Accuracy: 0.9345\n","Epoch: 45, Accuracy: 0.935\n","Epoch: 46, Accuracy: 0.9354\n","Epoch: 47, Accuracy: 0.9356\n","Epoch: 48, Accuracy: 0.9359\n","Epoch: 49, Accuracy: 0.9364\n"]}],"source":["def sigmoid(z):\n","    return 1.0 / (1.0 + np.exp(-z))\n","\n","def sigmoid_prime(z):\n","    # Derivative of the sigmoid\n","    return sigmoid(z) * (1 - sigmoid(z))\n","\n","class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x)\n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","    def feedforward(self, a):\n","        # Run the network on a single case\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(np.dot(w, a)+b)\n","        return a\n","\n","    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b = [np.zeros(b.shape) for b in self.biases]\n","        nabla_w = [np.zeros(w.shape) for w in self.weights]\n","        for x, y in zip(x_mini_batch, y_mini_batch):\n","            delta_nabla_b, delta_nabla_w = self.backprop(x.reshape(784,1), y.reshape(10,1))\n","            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n","            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n","        self.weights = [w - (eta / len(x_mini_batch)) * nw\n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b - (eta/len(x_mini_batch)) * nb\n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x, y):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","\n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = []\n","        delta_nabla_w = []\n","\n","        # Then go forward remembering all values before and after activations\n","        # in two other array list\n","        a = x\n","        v = x\n","        activations = [a]\n","        values = [a]\n","        for b, w in zip(self.biases, self.weights):\n","            v = np.dot(w, a) + b\n","            values.append(v)\n","            a = sigmoid(v)\n","            activations.append(a)\n","\n","        # Now go backward from the final cost applying backpropagation\n","        next_layer_gradient = sigmoid_prime(v) * self.cost_derivative(a, y)\n","        reversed_zip = list(reversed(list(zip(self.biases, self.weights, activations[:-1], values[:-1]))))\n","        for b, w, activation, val in reversed_zip:\n","            delta_nabla_b.append(np.array(next_layer_gradient).reshape(-1, 1))\n","            delta_nabla_w.append(np.array(next_layer_gradient).reshape(-1, 1) @ activation.reshape(1, -1))\n","            next_layer_gradient = sigmoid_prime(val) * (w.T @ next_layer_gradient.reshape(-1, 1))\n","\n","        delta_nabla_b = list(reversed(delta_nabla_b))\n","        delta_nabla_w = list(reversed(delta_nabla_w))\n","\n","        return delta_nabla_b, delta_nabla_w\n","\n","    def evaluate(self, x_test_data, y_test_data):\n","        # return accuracy\n","        return np.mean(np.argmax(self.feedforward(x_test_data), axis=0) == np.argmax(y_test_data, axis=0))\n","\n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations-y)\n","\n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n","                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test.T, y_test.T)))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=50, mini_batch_size=100, eta=3., test_data=(x_test, y_test))"]},{"cell_type":"code","source":["# Exercise 2, vectorized version\n","\n","def sigmoid(z):\n","    return 1.0 / (1.0 + np.exp(-z))\n","\n","def sigmoid_prime(z):\n","    # Derivative of the sigmoid\n","    return sigmoid(z) * (1 - sigmoid(z))\n","\n","class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x)\n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","\n","    def feedforward(self, a):\n","        # Run the network on multiple cases\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(np.dot(w, a) + b)\n","        return a\n","\n","    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        delta_nabla_b, delta_nabla_w = self.backprop(x_mini_batch, y_mini_batch)\n","        self.weights = [w - (eta / x_mini_batch.shape[1]) * nw\n","                        for w, nw in zip(self.weights, delta_nabla_w)]\n","        self.biases = [b - (eta / x_mini_batch.shape[1]) * nb\n","                       for b, nb in zip(self.biases, delta_nabla_b)]\n","\n","    def backprop(self, x, y):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","\n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = []\n","        delta_nabla_w = []\n","\n","        # Then go forward remembering all values before and after activations\n","        # in two other array list\n","        a = x\n","        v = x\n","        activations = [a]\n","        values = [a]\n","        for b, w in zip(self.biases, self.weights):\n","            v = np.dot(w, a) + b\n","            values.append(v)\n","            a = sigmoid(v)\n","            activations.append(a)\n","\n","        # Now go backward from the final cost applying backpropagation\n","        next_layer_gradient = self.cost_derivative(a, y) * sigmoid_prime(v)\n","        reversed_zip = list(reversed(list(zip(self.biases, self.weights, activations[:-1], values[:-1]))))\n","        for b, w, activation, val in reversed_zip:\n","            delta_nabla_b.append(np.sum(np.array(next_layer_gradient), axis=1).reshape(b.shape))\n","            delta_nabla_w.append((np.array(next_layer_gradient) @ activation.T).reshape(w.shape))\n","            next_layer_gradient = (w.T @ next_layer_gradient) * sigmoid_prime(val)\n","\n","        delta_nabla_b = list(reversed(delta_nabla_b))\n","        delta_nabla_w = list(reversed(delta_nabla_w))\n","\n","        return delta_nabla_b, delta_nabla_w\n","\n","    def evaluate(self, x_test_data, y_test_data):\n","        # return accuracy\n","        return np.mean(np.argmax(self.feedforward(x_test_data), axis=0) == np.argmax(y_test_data, axis=0))\n","\n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations - y)\n","\n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n","                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n","                self.update_mini_batch(x_mini_batch.T, y_mini_batch.T, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test.T, y_test.T)))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784, 30, 30, 10])\n","network.SGD((x_train, y_train), epochs=50, mini_batch_size=100, eta=3., test_data=(x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6uwumOKeqX9","executionInfo":{"status":"ok","timestamp":1698919042122,"user_tz":-60,"elapsed":190734,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}},"outputId":"7f5c99bc-27d0-4fd0-a7e0-3ddf0b543dc3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.7839\n","Epoch: 1, Accuracy: 0.8506\n","Epoch: 2, Accuracy: 0.8779\n","Epoch: 3, Accuracy: 0.8906\n","Epoch: 4, Accuracy: 0.8994\n","Epoch: 5, Accuracy: 0.9053\n","Epoch: 6, Accuracy: 0.9112\n","Epoch: 7, Accuracy: 0.9145\n","Epoch: 8, Accuracy: 0.9168\n","Epoch: 9, Accuracy: 0.9213\n","Epoch: 10, Accuracy: 0.923\n","Epoch: 11, Accuracy: 0.9248\n","Epoch: 12, Accuracy: 0.9268\n","Epoch: 13, Accuracy: 0.9283\n","Epoch: 14, Accuracy: 0.9293\n","Epoch: 15, Accuracy: 0.931\n","Epoch: 16, Accuracy: 0.9337\n","Epoch: 17, Accuracy: 0.9351\n","Epoch: 18, Accuracy: 0.936\n","Epoch: 19, Accuracy: 0.9361\n","Epoch: 20, Accuracy: 0.936\n","Epoch: 21, Accuracy: 0.9369\n","Epoch: 22, Accuracy: 0.9374\n","Epoch: 23, Accuracy: 0.9383\n","Epoch: 24, Accuracy: 0.9387\n","Epoch: 25, Accuracy: 0.9397\n","Epoch: 26, Accuracy: 0.9403\n","Epoch: 27, Accuracy: 0.941\n","Epoch: 28, Accuracy: 0.9411\n","Epoch: 29, Accuracy: 0.9415\n","Epoch: 30, Accuracy: 0.942\n","Epoch: 31, Accuracy: 0.9427\n","Epoch: 32, Accuracy: 0.9435\n","Epoch: 33, Accuracy: 0.944\n","Epoch: 34, Accuracy: 0.9439\n","Epoch: 35, Accuracy: 0.9438\n","Epoch: 36, Accuracy: 0.944\n","Epoch: 37, Accuracy: 0.9442\n","Epoch: 38, Accuracy: 0.9439\n","Epoch: 39, Accuracy: 0.9446\n","Epoch: 40, Accuracy: 0.9448\n","Epoch: 41, Accuracy: 0.9447\n","Epoch: 42, Accuracy: 0.9454\n","Epoch: 43, Accuracy: 0.9459\n","Epoch: 44, Accuracy: 0.9462\n","Epoch: 45, Accuracy: 0.9464\n","Epoch: 46, Accuracy: 0.9468\n","Epoch: 47, Accuracy: 0.9475\n","Epoch: 48, Accuracy: 0.9478\n","Epoch: 49, Accuracy: 0.9479\n"]}]},{"cell_type":"markdown","metadata":{"id":"eOlAAx-uXOvi"},"source":["# Excercise 3 (optional)\n","\n","Standart backpropagation method requires memorization of all outputs of all layers, which can take much of precious GPU memory.\n","Instead of doing that, one can memorize only a select few layers and then recompute the rest as they are needed.\n","Your task is to complete the code below to implement backpropagation with checkpoints.\n","To keep things simple, use 1-example mini-batches (or, if you are bored, vectorize the code below)"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"4ks-sxtd6VrY","executionInfo":{"status":"ok","timestamp":1698922020827,"user_tz":-60,"elapsed":221,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}}},"outputs":[],"source":["class NetworkWithCheckpoints(object):\n","    def __init__(self, sizes, checkpoints):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes) - 1\n","        self.sizes = sizes\n","        self.checkpoints = checkpoints\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x)\n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","\n","    def feedforward(self, a):\n","        # Run the network on a single case\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(np.dot(w, a)+b)\n","        return a\n","\n","    def feedforward_with_checkpoints(self, x):\n","        # Runs network on a single case, memorizing the activations of layers included in checkpoints.\n","        # Notice that gs (outputs of non-linearities) are shifted by one\n","        fs = []\n","        gs = [x]\n","        g = x\n","        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n","            f = np.dot(w, g) + b\n","            g = sigmoid(f)\n","            if i in self.checkpoints:\n","                fs.append(f)\n","                gs.append(g)\n","            else:\n","                fs.append(None)\n","                gs.append(None)\n","        return fs, gs, g\n","\n","\n","    def feedforward_between_layers(self, first_layer: int, last_layer: int, acc_f, acc_g):\n","        # feedforward input acc_g[first_layer] for layers [first_layer, last_layer)\n","        # memorizing their outputs in respective indexes of acc_f, acc_g\n","        g = acc_g[first_layer]\n","        for x in range(first_layer + 1, last_layer + 1, 1):\n","            f = np.dot(self.weights[x - 1], g) + self.biases[x - 1]\n","            g = sigmoid(f)\n","            acc_f[x - 1] = f\n","            acc_g[x] = g\n","\n","        pass\n","\n","    def backprop_between_layers(self, start, end, acc_f, acc_g, dLdg):\n","        # compute the gradients for layers [start, end)\n","        # dLdg is a gradient with respect to nonlinearity of layer[end-1]\n","        # return changed dLdG so that it is gradient with respect to nonlinearieties of layer start-1\n","        dLdWs = []\n","        dLdBs = []\n","\n","        for x in reversed(range(start - 1, end - 1, 1)):\n","          dLdg_with_lin = sigmoid_prime(acc_f[x + 1]) * dLdg\n","          dLdBs.append(dLdg_with_lin.reshape(-1, 1))\n","          dLdWs.append(dLdg_with_lin.reshape(-1, 1) @ acc_g[x + 1].reshape(1, -1))\n","          dLdg = self.weights[x + 1].T @ dLdg_with_lin.reshape(-1, 1)\n","\n","        return reversed(dLdWs), reversed(dLdBs), dLdg\n","\n","    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation with checkpoints to compute the gradient.\n","        # For this exercise, we assume 1 element mini_batch\n","        # eta is the learning rate\n","        x_mini_batch = x_mini_batch.reshape(-1, 1)\n","        y_mini_batch = y_mini_batch.reshape(-1, 1)\n","\n","        fs, gs, output = self.feedforward_with_checkpoints(x_mini_batch)\n","        dLdg = output - y_mini_batch\n","        for start, end in reversed(list(zip([-1] + self.checkpoints, self.checkpoints + [self.num_layers-1]))):\n","            # if checkpoints are (a, b) then we can backprop through layers [a+1, b] inclusive\n","            start += 1\n","            end += 1\n","            # those copies are inefficient, but we do them to keep indexing simple\n","            acc_f = fs.copy()\n","            acc_g = gs.copy()\n","            self.feedforward_between_layers(start, end, acc_f, acc_g)\n","            nabla_w, nabla_b, dLdg = self.backprop_between_layers(start, end, acc_f, acc_g, dLdg)\n","            self.weights[start:end] = [w - eta * dw for w, dw in zip(self.weights[start:end], nabla_w)]\n","            self.biases[start:end] = [b - eta * db for b, db in zip(self.biases[start:end], nabla_b)]\n","\n","\n","    def evaluate(self, x_test_data, y_test_data):\n","        # return accuracy\n","        return np.mean(np.argmax(self.feedforward(x_test_data), axis=0) == np.argmax(y_test_data, axis=0))\n","\n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations-y)\n","\n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n","                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test.T, y_test.T)))\n","            else:\n","                print(\"Epoch: {0}\".format(j))"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"YzT_Bd4PXOvj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698922886169,"user_tz":-60,"elapsed":339906,"user":{"displayName":"Michał Traczyk","userId":"00555487641805485616"}},"outputId":"7e908a71-ed71-41d0-dbc2-27904f06adfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8842\n","Epoch: 1, Accuracy: 0.9013\n","Epoch: 2, Accuracy: 0.907\n","Epoch: 3, Accuracy: 0.8987\n","Epoch: 4, Accuracy: 0.9125\n","Epoch: 5, Accuracy: 0.9067\n","Epoch: 6, Accuracy: 0.9209\n","Epoch: 7, Accuracy: 0.9283\n","Epoch: 8, Accuracy: 0.9218\n","Epoch: 9, Accuracy: 0.9198\n"]}],"source":["network = NetworkWithCheckpoints([784, 30, 30, 10], checkpoints=[1])\n","network.SGD((x_train, y_train), epochs=10, mini_batch_size=1, eta=1., test_data=(x_test, y_test) )"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mim-uw/dnn-2023-24/blob/master/docs/DNN-Lab-2-backprop-student-version.ipynb","timestamp":1698834627497}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}